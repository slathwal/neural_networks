<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shefali Lathwal">
<meta name="dcterms.date" content="2025-06-12">

<title>Developing a character-prediction n-gram model using MLP - version 3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="character_level_ngram_model_v3_files/libs/clipboard/clipboard.min.js"></script>
<script src="character_level_ngram_model_v3_files/libs/quarto-html/quarto.js"></script>
<script src="character_level_ngram_model_v3_files/libs/quarto-html/popper.min.js"></script>
<script src="character_level_ngram_model_v3_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="character_level_ngram_model_v3_files/libs/quarto-html/anchor.min.js"></script>
<link href="character_level_ngram_model_v3_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="character_level_ngram_model_v3_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="character_level_ngram_model_v3_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="character_level_ngram_model_v3_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="character_level_ngram_model_v3_files/libs/bootstrap/bootstrap-973236bd072d72a04ee9cd82dcc9cb29.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#import-dependencies" id="toc-import-dependencies" class="nav-link active" data-scroll-target="#import-dependencies">Import dependencies</a></li>
  <li><a href="#read-the-data-file-and-create-a-mapping-for-all-characters" id="toc-read-the-data-file-and-create-a-mapping-for-all-characters" class="nav-link" data-scroll-target="#read-the-data-file-and-create-a-mapping-for-all-characters">Read the data file and create a mapping for all characters</a></li>
  <li><a href="#set-some-parameters" id="toc-set-some-parameters" class="nav-link" data-scroll-target="#set-some-parameters">Set some parameters</a></li>
  <li><a href="#build-the-training-validation-and-test-datasets" id="toc-build-the-training-validation-and-test-datasets" class="nav-link" data-scroll-target="#build-the-training-validation-and-test-datasets">Build the training, validation and test datasets</a></li>
  <li><a href="#initialize-the-neural-network" id="toc-initialize-the-neural-network" class="nav-link" data-scroll-target="#initialize-the-neural-network">Initialize the neural network</a></li>
  <li><a href="#train-the-neural-network" id="toc-train-the-neural-network" class="nav-link" data-scroll-target="#train-the-neural-network">Train the neural network</a></li>
  <li><a href="#evaluation-code" id="toc-evaluation-code" class="nav-link" data-scroll-target="#evaluation-code">Evaluation code</a></li>
  <li><a href="#sample-from-the-model" id="toc-sample-from-the-model" class="nav-link" data-scroll-target="#sample-from-the-model">Sample from the model</a></li>
  <li><a href="#expected-loss-at-initialization" id="toc-expected-loss-at-initialization" class="nav-link" data-scroll-target="#expected-loss-at-initialization">Expected loss at initialization</a></li>
  <li><a href="#inspect-and-play-with-the-logits-and-other-intermediate-values-at-initialization" id="toc-inspect-and-play-with-the-logits-and-other-intermediate-values-at-initialization" class="nav-link" data-scroll-target="#inspect-and-play-with-the-logits-and-other-intermediate-values-at-initialization">Inspect and play with the logits and other intermediate values at initialization</a></li>
  <li><a href="#how-to-set-scales-for-layer-weights-that-help-with-less-tanh-saturation-and-smaller-logit-values-at-initialization" id="toc-how-to-set-scales-for-layer-weights-that-help-with-less-tanh-saturation-and-smaller-logit-values-at-initialization" class="nav-link" data-scroll-target="#how-to-set-scales-for-layer-weights-that-help-with-less-tanh-saturation-and-smaller-logit-values-at-initialization">How to set scales for layer weights that help with less tanh saturation and smaller logit values at initialization</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">Batch normalization</a></li>
  <li><a href="#tracking-losses" id="toc-tracking-losses" class="nav-link" data-scroll-target="#tracking-losses">Tracking losses</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Developing a character-prediction n-gram model using MLP - version 3</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Shefali Lathwal </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 12, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">June 13, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p>In this notebook, we will work on the following goals:</p>
<ul>
<li><p>Understand the activation steps within a neural network and the gradients that are flowing back through the network</p></li>
<li><p>How to do effective initialization of weights</p></li>
</ul>
<section id="import-dependencies" class="level1">
<h1>Import dependencies</h1>
<div id="9e3af0f0" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="read-the-data-file-and-create-a-mapping-for-all-characters" class="level1">
<h1>Read the data file and create a mapping for all characters</h1>
<div id="4241b545" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">"data/names.txt"</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> <span class="bu">file</span>.read().splitlines()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="bu">file</span>.close()</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Total words in the data:"</span>,<span class="bu">len</span>(words))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the lookup for character indices including a start and end of line character to the all_chars list</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>all_chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(<span class="st">""</span>.join(words)))<span class="op">+</span>[<span class="st">"*"</span>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>stoi <span class="op">=</span> {s:i <span class="cf">for</span> i, s <span class="kw">in</span> <span class="bu">enumerate</span>(all_chars)}</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>itos  <span class="op">=</span> {i:s <span class="cf">for</span> s, i <span class="kw">in</span> stoi.items()}</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>stoi<span class="op">=</span><span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>itos<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Total words in the data: 55649
stoi={' ': 0, '*': 1, '-': 2, '.': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29}
itos={0: ' ', 1: '*', 2: '-', 3: '.', 4: 'a', 5: 'b', 6: 'c', 7: 'd', 8: 'e', 9: 'f', 10: 'g', 11: 'h', 12: 'i', 13: 'j', 14: 'k', 15: 'l', 16: 'm', 17: 'n', 18: 'o', 19: 'p', 20: 'q', 21: 'r', 22: 's', 23: 't', 24: 'u', 25: 'v', 26: 'w', 27: 'x', 28: 'y', 29: 'z'}</code></pre>
</div>
</div>
</section>
<section id="set-some-parameters" class="level1">
<h1>Set some parameters</h1>
<div id="57f95266" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(all_chars) <span class="co"># vocabulary size</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>context_length <span class="op">=</span> <span class="dv">3</span> <span class="co"># no of characters in context for prediction</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>embed_size <span class="op">=</span> <span class="dv">10</span> <span class="co"># embedding vector length for each character</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">200</span> <span class="co"># number of neurons in the hidden layer</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>min_batch_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># batch size in each gradient descent iteration</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Print parameters</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>vocab_size<span class="op">=</span><span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>context_length<span class="op">=</span><span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>embed_size<span class="op">=</span><span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>n_hidden<span class="op">=</span><span class="sc">}</span><span class="ch">\n</span><span class="sc">{</span>min_batch_size<span class="op">=</span><span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>vocab_size=30
context_length=3
embed_size=10
n_hidden=200
min_batch_size=32</code></pre>
</div>
</div>
</section>
<section id="build-the-training-validation-and-test-datasets" class="level1">
<h1>Build the training, validation and test datasets</h1>
<div id="f112bd4b" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_dataset(words, context_length <span class="op">=</span> context_length):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    xs, ys <span class="op">=</span> [], []</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        chs <span class="op">=</span> <span class="st">"*"</span><span class="op">*</span>context_length<span class="op">+</span>word<span class="op">+</span><span class="st">"*"</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="co">#print(word)</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> ind <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(chs) <span class="op">-</span> context_length):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>            ch1 <span class="op">=</span> chs[ind:ind<span class="op">+</span>context_length]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>            ch2 <span class="op">=</span> chs[ind<span class="op">+</span>context_length]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            <span class="co">#print(f"{ch1} ----&gt; {ch2}")</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            xs.append([stoi[ch] <span class="cf">for</span> ch <span class="kw">in</span> ch1])</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            ys.append(stoi[ch2])</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> torch.tensor(xs) <span class="co"># 10X3</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> torch.tensor(ys) <span class="co"># 10</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(xs.shape, ys.shape)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xs, ys</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>random.shuffle(words)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>n1 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span><span class="op">*</span><span class="bu">len</span>(words))</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>n2 <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(words))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>n1, n2</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>xstr, ystr <span class="op">=</span> build_dataset(words[:n1])</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>xsdev, ysdev <span class="op">=</span> build_dataset(words[n1:n2])</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>xstest, ystest <span class="op">=</span> build_dataset(words[n2:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([404769, 3]) torch.Size([404769])
torch.Size([50471, 3]) torch.Size([50471])
torch.Size([50452, 3]) torch.Size([50452])</code></pre>
</div>
</div>
</section>
<section id="initialize-the-neural-network" class="level1">
<h1>Initialize the neural network</h1>
<div id="37ea4700" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">4524757136458</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.randn(size <span class="op">=</span> (vocab_size, embed_size), generator<span class="op">=</span>g)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn(size <span class="op">=</span> (context_length <span class="op">*</span> embed_size,n_hidden), generator<span class="op">=</span>g)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> torch.randn(size <span class="op">=</span> (n_hidden,), generator<span class="op">=</span>g)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn(size <span class="op">=</span> (n_hidden, vocab_size), generator<span class="op">=</span>g)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.rand(size <span class="op">=</span> (vocab_size,), generator<span class="op">=</span>g)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># collect all the parameters</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of parameters:"</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of parameters: 12530</code></pre>
</div>
</div>
</section>
<section id="train-the-neural-network" class="level1">
<h1>Train the neural network</h1>
<div id="6579aba7" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>n_iters <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lossi <span class="op">=</span> []</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iters): </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># construct minibatch by sampling indices from training dataset</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, xstr.shape[<span class="dv">0</span>], (min_batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    Xb, Yb <span class="op">=</span> xstr[ix], ystr[ix] <span class="co"># Batch X, Y </span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward pass</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    xemb <span class="op">=</span> C[Xb].view(min_batch_size, context_length<span class="op">*</span>embed_size) <span class="co"># embed the characters into vectors and concatenate the vectors</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> (xemb <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># hidden layer pre-activation</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">%</span><span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Loss at iteration </span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss"> /</span><span class="sc">{</span>n_iters<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update parameters</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">10000</span>:</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">10000</span> <span class="op">&lt;=</span> i <span class="op">&lt;</span> <span class="dv">50000</span>:</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate<span class="op">/</span><span class="dv">10</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate<span class="op">/</span><span class="dv">100</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr<span class="op">*</span>p.grad</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># track statistics</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    lossi.append(loss.item())</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co">#The above loss is just for the minibatch. Calculating loss for the full training data and validation data later.</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss at iteration       0 / 200000: 23.6941
Loss at iteration   10000 / 200000: 2.8908
Loss at iteration   20000 / 200000: 2.0903
Loss at iteration   30000 / 200000: 2.0194
Loss at iteration   40000 / 200000: 1.8330
Loss at iteration   50000 / 200000: 2.2588
Loss at iteration   60000 / 200000: 2.1012
Loss at iteration   70000 / 200000: 2.2988
Loss at iteration   80000 / 200000: 1.9204
Loss at iteration   90000 / 200000: 1.9707
Loss at iteration  100000 / 200000: 1.9875
Loss at iteration  110000 / 200000: 2.1502
Loss at iteration  120000 / 200000: 1.8993
Loss at iteration  130000 / 200000: 1.9328
Loss at iteration  140000 / 200000: 2.0698
Loss at iteration  150000 / 200000: 1.7200
Loss at iteration  160000 / 200000: 2.0529
Loss at iteration  170000 / 200000: 1.8471
Loss at iteration  180000 / 200000: 2.1751
Loss at iteration  190000 / 200000: 2.0163
1.834747552871704</code></pre>
</div>
</div>
<div id="d9134bba" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># It can be useful to plot logloss instead of loss</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.plot(np.arange(n_iters), np.log10(lossi))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"iteration"</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Log loss"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>Text(0, 0.5, 'Log loss')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-8-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="evaluation-code" class="level1">
<h1>Evaluation code</h1>
<ul>
<li>Calculate the loss for selected split of the data - train, dev, or test</li>
</ul>
<div id="6428809c" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We can also use a context manager (look this up)</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_split_loss(split):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the appropriate x an y based on the desired data split</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    x,y <span class="op">=</span> {</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"train"</span>: (xstr, ystr),</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"val"</span>:(xsdev, ysdev),</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"test"</span>: (xstest, ystest)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    }[split]</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    xemb <span class="op">=</span> C[x].view(<span class="op">-</span><span class="dv">1</span>,context_length<span class="op">*</span>embed_size)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#xembcat = xemb</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    xh <span class="op">=</span> (xemb <span class="op">@</span> W1 <span class="op">+</span> b1).tanh()</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> xh <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(split, loss.item())</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>calculate_split_loss(<span class="st">'train'</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>calculate_split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train 2.01993989944458
val 2.0429399013519287</code></pre>
</div>
</div>
</section>
<section id="sample-from-the-model" class="level1">
<h1>Sample from the model</h1>
<div id="72b9ddb8" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>start_char <span class="op">=</span> <span class="st">"*"</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>end_char <span class="op">=</span> <span class="st">"*"</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">123434</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>start_ind <span class="op">=</span> stoi[start_char]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    context <span class="op">=</span> [start_ind]<span class="op">*</span>context_length</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    all_chars <span class="op">=</span> [start_char]<span class="op">*</span>context_length</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:  </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now context contains the three character indices for our example</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        xembp <span class="op">=</span> C[torch.tensor(context)].view(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># Want to generate just one character at a time.</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>        xhp <span class="op">=</span> (xembp <span class="op">@</span> W1 <span class="op">+</span> b1).tanh() </span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        logitsp <span class="op">=</span> xhp <span class="op">@</span> W2 <span class="op">+</span> b2 </span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Directly use the softmax function to calculate probabilites from logits</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        probsp <span class="op">=</span> F.softmax(logitsp, dim <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        ind <span class="op">=</span> torch.multinomial(probsp, num_samples <span class="op">=</span> <span class="dv">1</span>, replacement <span class="op">=</span> <span class="va">True</span>, generator <span class="op">=</span> g)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        ch <span class="op">=</span> itos[ind.item()]</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (ch <span class="op">==</span> end_char):</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>            all_chars.append(ch)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:   </span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>            all_chars.append(ch)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            context <span class="op">=</span> context[<span class="dv">1</span>:]<span class="op">+</span>[ind.item()]</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">""</span>.join(all_chars[<span class="dv">2</span>:]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>*tevanshi*
*kashivik*
*matararavu*
*aash*
*theralini*
*rini*
*baraj*
*aswitha*
*gar*
*san*
*gugan*
*harnisheesh*
*enpugaree*
*agrikharnapeeva*
*mathurdhalakiya*
*yani*
*tarenashwaduban*
*bany*
*druswa*
*grith*</code></pre>
</div>
</div>
</section>
<section id="expected-loss-at-initialization" class="level1">
<h1>Expected loss at initialization</h1>
<div id="d515886a" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="op">-</span>torch.tensor(<span class="fl">1.</span><span class="op">/</span>vocab_size).log()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor(3.4012)</code></pre>
</div>
</div>
<p>We expect a loss of about 3.4 at initialization, but right now we are getting a loss of about 27</p>
<p>If our logits are big numbers, we start to get very high losses. Therefore, at initialization, we want logits to either be equal for all output possibilities or close to and distributed around zero and not arbitrarily high numbers.</p>
</section>
<section id="inspect-and-play-with-the-logits-and-other-intermediate-values-at-initialization" class="level1">
<h1>Inspect and play with the logits and other intermediate values at initialization</h1>
<p>Re-initialize the neural network</p>
<div id="bcf81734" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">4524757136458</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.randn(size <span class="op">=</span> (vocab_size, embed_size), generator<span class="op">=</span>g)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn(size <span class="op">=</span> (context_length <span class="op">*</span> embed_size,n_hidden), generator<span class="op">=</span>g) <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>)<span class="op">/</span>((context_length<span class="op">*</span>embed_size)<span class="op">**</span><span class="fl">0.5</span>) <span class="co">#0.2</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> torch.randn(size <span class="op">=</span> (n_hidden,), generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn(size <span class="op">=</span> (n_hidden, vocab_size), generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.rand(size <span class="op">=</span> (vocab_size,), generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, b1, W2, b2]</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># collect all the parameters</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of parameters:"</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of parameters: 12530</code></pre>
</div>
</div>
<p>Training loop</p>
<div id="4f218f8b" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>n_iters <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>lossi <span class="op">=</span> []</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iters): </span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># construct minibatch by sampling indices from training dataset</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, xstr.shape[<span class="dv">0</span>], (min_batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    Xb, Yb <span class="op">=</span> xstr[ix], ystr[ix] <span class="co"># Batch X, Y </span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward pass</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    xemb <span class="op">=</span> C[Xb].view(min_batch_size, context_length<span class="op">*</span>embed_size) <span class="co"># embed the characters into vectors and concatenate the vectors</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> (xemb <span class="op">@</span> W1 <span class="op">+</span> b1) <span class="co"># hidden layer pre-activation</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">%</span><span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Loss at iteration </span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss"> /</span><span class="sc">{</span>n_iters<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update parameters</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">10000</span>:</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">10000</span> <span class="op">&lt;=</span> i <span class="op">&lt;</span> <span class="dv">50000</span>:</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate<span class="op">/</span><span class="dv">10</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate<span class="op">/</span><span class="dv">100</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr<span class="op">*</span>p.grad</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># track statistics</span></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    lossi.append(loss.item())</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>    <span class="co">#break</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss at iteration       0 / 200000: 3.3886
Loss at iteration   10000 / 200000: 2.2922
Loss at iteration   20000 / 200000: 1.9685
Loss at iteration   30000 / 200000: 1.9407
Loss at iteration   40000 / 200000: 1.7619
Loss at iteration   50000 / 200000: 2.0589
Loss at iteration   60000 / 200000: 1.9139
Loss at iteration   70000 / 200000: 2.2412
Loss at iteration   80000 / 200000: 1.7218
Loss at iteration   90000 / 200000: 1.9227
Loss at iteration  100000 / 200000: 1.8951
Loss at iteration  110000 / 200000: 2.0498
Loss at iteration  120000 / 200000: 1.7472
Loss at iteration  130000 / 200000: 1.8809
Loss at iteration  140000 / 200000: 2.0456
Loss at iteration  150000 / 200000: 1.6247
Loss at iteration  160000 / 200000: 1.8501
Loss at iteration  170000 / 200000: 1.6803
Loss at iteration  180000 / 200000: 1.9745
Loss at iteration  190000 / 200000: 1.9393
1.6462198495864868</code></pre>
</div>
</div>
<p>we can look at the output coming out of the hidden layer as an image. In the image, white represents true and black represents false. There shouldnt be too much white in this.</p>
<div id="ceb6010f" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">20</span>,<span class="dv">10</span>))</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(h.<span class="bu">abs</span>() <span class="op">&gt;</span> <span class="fl">0.99</span>, cmap <span class="op">=</span> <span class="st">'gray'</span>, interpolation <span class="op">=</span> <span class="st">'nearest'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="11ce93fb" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>plt.plot(np.log10(lossi))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ce0e69be" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>calculate_split_loss(<span class="st">'train'</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>calculate_split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train 1.9266620874404907
val 1.9444915056228638</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I am re-running cells in the notebook, so the output of the cells below will not show the logits and intermediate values obtained without any modification of parameter initialization.</p>
</div>
</div>
<p>Look at logit values</p>
<div id="5eb3f90d" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>logits[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>tensor([-0.5982,  3.6528, -2.8297, -2.2792,  5.4065, -1.6365, -1.7818, -1.1617,
         3.6341, -3.5775, -2.0176, -0.5987,  5.7543, -0.4845, -0.5263, -0.9710,
         1.0501,  1.4979,  0.3084, -1.1211, -3.1393,  1.9131,  0.6301,  0.4942,
         2.9674,  1.1781, -0.7974, -4.0875,  3.0433, -3.8424],
       grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
<p>Look at values of h</p>
<div id="2785adae" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>h.shape <span class="co"># h is 32Xn_hidden because size of mini-batch is 32</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>h.view(<span class="op">-</span><span class="dv">1</span>).tolist() <span class="co"># Create a list</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"values coming out of the hidden layer"</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>plt.hist(h.view(<span class="op">-</span><span class="dv">1</span>).tolist(), bins <span class="op">=</span> <span class="dv">50</span>)<span class="op">;</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>plt.hist(hpreact.view(<span class="op">-</span><span class="dv">1</span>).tolist(), bins <span class="op">=</span> <span class="dv">50</span>)</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"value of preactivations feeding into the tanh function"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Text(0.5, 1.0, 'value of preactivations feeding into the tanh function')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-18-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-18-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Logits: - If we look at logits from random uniform initialization, well see that the numbers are very high, as much as 18. Since logit calculation depends on W2 and b2, we can fix this by reducing W2 and b2.</p>
<ul>
<li><p>If I scale down W2 by 0.01 and make b2=0, I get an initial loss of about 3.43, which is close to what we expect from uniform distribution of probabiltiies</p></li>
<li><p>We dont want weights of a neural network layer exactly to be zero because </p></li>
<li><p>Just by initializing the logits to be small, we can get the training loss and validation loss down from 2.024 and 2.044 to 1.942 and 1.957</p></li>
</ul>
<p>h - output of hidden layer: - If we look at values of h, we will see that too many values are in the flat regions of tanh, i.e., -1 or +1. In these regions, gradients are approximately zero and therefore will not affect parameters much. - The values coming from tanh are in the +1 and -1 region, because the preactivations feeding into the tanh function take on very large positive and negative values. - if for any neuron, all the inputs turn out to be in the flat region of tanh, it would never learn in that batch and it would be a dead neuron, because all gradients would be zero. - We can reduce the values of preactivation by reducing W1 and b1 - Fixing the above reduces loss a little bit as well to 1.938 for train and 1.950 for val. - We are improving on losses because due to better initialization, we spend more cycles actually optimizing than fixing initialization issues.</p>
<ul>
<li>With deeper networks - example 50 layers - these problems stack up and deeper networks become less forgiving to problems like initialization.</li>
</ul>
</section>
<section id="how-to-set-scales-for-layer-weights-that-help-with-less-tanh-saturation-and-smaller-logit-values-at-initialization" class="level1">
<h1>How to set scales for layer weights that help with less tanh saturation and smaller logit values at initialization</h1>
<p>Visualize the problem first.</p>
<p>Look at statistics of gaussian distributions at initialization and when they go through computations like matrix multiplication</p>
<div id="141a360a" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random numbers first</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">10</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">@</span> W</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.mean(), x.std())</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(W.mean(), W.std())</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.mean(), y.std())</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, sharex <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(x.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density <span class="op">=</span> <span class="va">True</span>)<span class="op">;</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Initialization from normal distribution"</span>)<span class="op">;</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(y.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density <span class="op">=</span> <span class="va">True</span>)<span class="op">;</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Distribution after matrix multiplication of two parameters</span><span class="ch">\n</span><span class="st"> initialized independently from normal distribution"</span>)<span class="op">;</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(-0.0117) tensor(0.9936)
tensor(-0.0033) tensor(0.9988)
tensor(0.0127) tensor(3.1360)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>We see that after matrix multiplication, the standard deviation of the resulting distributin increases. Thereforem the question is how do we scale W to preserve standard deviation to be 1.</p>
<p>Mathematically, to preserve the standard deviation after matrix multiplication, we divide by the square root of <code>fan_in</code>, which is the number of input elements to the next layer</p>
<div id="e1732090" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">10</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">200</span>) <span class="op">/</span> <span class="dv">10</span><span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">@</span> W</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x.mean(), x.std())</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(W.mean(), W.std())</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y.mean(), y.std())</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">1</span>, sharex <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].hist(x.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density <span class="op">=</span> <span class="va">True</span>)<span class="op">;</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>].set_title(<span class="st">"Initialization from normal distribution"</span>)<span class="op">;</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].hist(y.view(<span class="op">-</span><span class="dv">1</span>).tolist(), <span class="dv">50</span>, density <span class="op">=</span> <span class="va">True</span>)<span class="op">;</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">1</span>].set_title(<span class="st">"Distribution after matrix multiplication of two parameters</span><span class="ch">\n</span><span class="st"> initialized independently from normal distribution </span><span class="ch">\n</span><span class="st"> after scaling the weights"</span>)<span class="op">;</span></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor(-0.0042) tensor(1.0093)
tensor(-0.0041) tensor(0.3148)
tensor(-0.0020) tensor(1.0038)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-20-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>A paper that explains these scaling factors to maintain good distributions throughout a neural network during initialization is Kaiming He et al.&nbsp;2015. They studied convolutional neural networs and they studied ReLU and PReLU non-linearities. In their analysis of forward activation of the neural net, they find that instead of reducing weights by <code>sqrt(fan_in)</code>, they also have to multiply by a <code>gain</code> to account for half of the numbers being discarded in ReLU.</p>
<p>Kaiming normalization is also implemented in pytorch in <code>torch.nn.init.kaiming_normal</code>. The documentation page for this function provides the gains for different non-linearities.</p>
<p>Intuitively, we need a gain because non-linearities like tanh or ReLU are squasing functions that narrow the distribution. Therefore, we need a small gain to counter the squashing.</p>
<p>There are modern techniques that counter some of the issues with incorrect normalizations: - residul connections - normalization layers like batch normalizations, layer normalizations etc. - better optimizers like Adam.</p>
<p>In practice, scale the std of weights by <code>gain/sqrt(fan_in)</code>, which means multiplying normally initialized weights by the factor above.</p>
</section>
<section id="batch-normalization" class="level1">
<h1>Batch normalization</h1>
<ul>
<li><p>Mititgates some of the issues associated with incorrect initialization.</p></li>
<li><p>We want all intermediate pre-activation states to neither be too small, because then tanh layer is not doing much or too large, because then tanh layer is saturated. We want these to be roughly Gaussian with unit standard deviation and zero mean at initialization.</p></li>
<li><p>Batch normalization says that we should directly normalize the pre-activate states to be unit Gaussian.</p></li>
<li><p>We can do the above, because standardization hidden pre-activation states, i.e., subtracting the mean and dividing by standard deviation is a differentiable operation.</p></li>
<li><p>We want to do this standardization at initialization but not at every training loop because we want the pre-activations to move around as needed during the training loop. So we use the concept of scale and shift by introducing two new parameters - gain and bias.</p></li>
<li><p>Simple batch normalization takes longer to train than a neural net without batch normalization.</p></li>
<li><p>its customary to add batch norm layers after every linear layer because it significantly stabilizes the training.</p></li>
<li><p>Batch normalization couples different training examples(in the mini-batch) mathematically. We can think of this as regularization because this coupling pads out the effect of any one example based on whatever other examples happen to be in the batch with it. This makes it hard for neural net to overfit to any given training example.</p></li>
<li><p>batch normalization can cause a lot of bugs in the code because training examples are coupled. Therefore, other normalization techniques are used which do not couple training examples, but its property of regularizing effect makes it widely used.</p></li>
<li><p>Since during training, we are coupling all examples in a mini-batch to calculate a single mean and standard deviation, we have a problem. At validation time, we only feed in a single example, so how do we get the neural network to predict something with only one example and not a mini-batch? We can deal with the above in two ways:</p>
<ul>
<li>Calculate a bnmean and bnstd after the training is finished for the whole dataset and then use it during validation.</li>
<li>Keep a running log of bnmean and bnstd during training and use it later during validation. This approach is preferred in practice.</li>
</ul></li>
<li><p>The running mean and running standard deviation of the batchnorm layer are also called buffers as these are calculated and updated during training but not through backpropagation as with other parameters.</p></li>
<li><p>Since we are standardizing the preactivations, the initial bias we added will not be used and we dont want to use bias as bias is added by the batchnorm layer.</p></li>
</ul>
<p>Initialize the neural network</p>
<div id="b8c604bb" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">4524757136458</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> torch.randn(size <span class="op">=</span> (vocab_size, embed_size), generator<span class="op">=</span>g)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> torch.randn(size <span class="op">=</span> (context_length <span class="op">*</span> embed_size,n_hidden), generator<span class="op">=</span>g) <span class="op">*</span> (<span class="dv">5</span><span class="op">/</span><span class="dv">3</span>)<span class="op">/</span>((context_length<span class="op">*</span>embed_size)<span class="op">**</span><span class="fl">0.5</span>) <span class="co">#0.2</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co">#b1 = torch.randn(size = (n_hidden,), generator=g) * 0.01</span></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> torch.randn(size <span class="op">=</span> (n_hidden, vocab_size), generator<span class="op">=</span>g) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> torch.rand(size <span class="op">=</span> (vocab_size,), generator<span class="op">=</span>g) <span class="op">*</span> <span class="dv">0</span></span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>bngain <span class="op">=</span> torch.ones(size <span class="op">=</span> (<span class="dv">1</span>, n_hidden)) <span class="co"># Because these are 1 at initialization, we have unit gaussian distribution at initialization and the distribution can shift as training proceeds.</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>bnbias <span class="op">=</span> torch.zeros(size <span class="op">=</span> (<span class="dv">1</span>, n_hidden))</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>bnmean_running <span class="op">=</span> torch.zeros(size <span class="op">=</span> (<span class="dv">1</span>, n_hidden)) <span class="co"># at initialization, the mean of preactivations is roughly 0</span></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>bnstd_running <span class="op">=</span> torch.ones(size <span class="op">=</span> (<span class="dv">1</span>, n_hidden)) <span class="co"># at initialization the std of pre-activations is roughly 1</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> [C, W1, W2, b2, bngain, bnbias]</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="co"># collect all the parameters</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    p.requires_grad <span class="op">=</span> <span class="va">True</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of parameters:"</span>, <span class="bu">sum</span>(p.nelement() <span class="cf">for</span> p <span class="kw">in</span> parameters))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of parameters: 12730</code></pre>
</div>
</div>
<p>Training loop:</p>
<div id="cd958061" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>n_iters <span class="op">=</span> <span class="dv">200000</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>lossi <span class="op">=</span> []</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_iters): </span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># construct minibatch by sampling indices from training dataset</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="dv">0</span>, xstr.shape[<span class="dv">0</span>], (min_batch_size,), generator<span class="op">=</span>g)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>    Xb, Yb <span class="op">=</span> xstr[ix], ystr[ix] <span class="co"># Batch X, Y </span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward pass</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># embedding layer</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a>    xemb <span class="op">=</span> C[Xb].view(min_batch_size, context_length<span class="op">*</span>embed_size) <span class="co"># embed the characters into vectors and concatenate the vectors</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear layer</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> xemb <span class="op">@</span> W1<span class="co"># + b1 # hidden layer pre-activation</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Batch Norm Layer</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># --------------------</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>    bnmeani <span class="op">=</span> hpreact.mean(dim <span class="op">=</span> <span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>) <span class="co"># bnmean ith iteration</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>    bnstdi <span class="op">=</span> hpreact.std(dim <span class="op">=</span> <span class="dv">0</span>, keepdim <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> bngain<span class="op">*</span>((hpreact <span class="op">-</span> bnmeani)<span class="op">/</span>(bnstdi)) <span class="op">+</span> bnbias <span class="co"># Standardize the pre-activation values to be unit gaussian</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad(): <span class="co"># do not compute gradients for these computations</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>        bnmean_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnmean_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnmeani</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>        bnstd_running <span class="op">=</span> <span class="fl">0.999</span> <span class="op">*</span> bnstd_running <span class="op">+</span> <span class="fl">0.001</span> <span class="op">*</span> bnstdi</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ------------------------</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># non-linearity</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> torch.tanh(hpreact) <span class="co"># hidden layer</span></span>
<span id="cb38-29"><a href="#cb38-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-30"><a href="#cb38-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output layer</span></span>
<span id="cb38-31"><a href="#cb38-31" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> h <span class="op">@</span> W2 <span class="op">+</span> b2 <span class="co"># output layer</span></span>
<span id="cb38-32"><a href="#cb38-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-33"><a href="#cb38-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># loss calculation</span></span>
<span id="cb38-34"><a href="#cb38-34" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, Yb) <span class="co"># loss function</span></span>
<span id="cb38-35"><a href="#cb38-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-36"><a href="#cb38-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">%</span><span class="dv">10000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb38-37"><a href="#cb38-37" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Loss at iteration </span><span class="sc">{</span>i<span class="sc">:7d}</span><span class="ss"> /</span><span class="sc">{</span>n_iters<span class="sc">:7d}</span><span class="ss">: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb38-38"><a href="#cb38-38" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb38-39"><a href="#cb38-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb38-40"><a href="#cb38-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb38-41"><a href="#cb38-41" aria-hidden="true" tabindex="-1"></a>        p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb38-42"><a href="#cb38-42" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb38-43"><a href="#cb38-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-44"><a href="#cb38-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update parameters</span></span>
<span id="cb38-45"><a href="#cb38-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="dv">10000</span>:</span>
<span id="cb38-46"><a href="#cb38-46" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate</span>
<span id="cb38-47"><a href="#cb38-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> <span class="dv">10000</span> <span class="op">&lt;=</span> i <span class="op">&lt;</span> <span class="dv">50000</span>:</span>
<span id="cb38-48"><a href="#cb38-48" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate<span class="op">/</span><span class="dv">10</span></span>
<span id="cb38-49"><a href="#cb38-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb38-50"><a href="#cb38-50" aria-hidden="true" tabindex="-1"></a>        lr <span class="op">=</span> learning_rate<span class="op">/</span><span class="dv">100</span></span>
<span id="cb38-51"><a href="#cb38-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> parameters:</span>
<span id="cb38-52"><a href="#cb38-52" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">+=</span> <span class="op">-</span>lr<span class="op">*</span>p.grad</span>
<span id="cb38-53"><a href="#cb38-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-54"><a href="#cb38-54" aria-hidden="true" tabindex="-1"></a>    <span class="co"># track statistics</span></span>
<span id="cb38-55"><a href="#cb38-55" aria-hidden="true" tabindex="-1"></a>    lossi.append(loss.item())</span>
<span id="cb38-56"><a href="#cb38-56" aria-hidden="true" tabindex="-1"></a>    <span class="co">#break</span></span>
<span id="cb38-57"><a href="#cb38-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-58"><a href="#cb38-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss at iteration       0 / 200000: 3.4106
Loss at iteration   10000 / 200000: 1.9552
Loss at iteration   20000 / 200000: 2.5670
Loss at iteration   30000 / 200000: 2.2542
Loss at iteration   40000 / 200000: 2.0727
Loss at iteration   50000 / 200000: 1.8331
Loss at iteration   60000 / 200000: 2.2747
Loss at iteration   70000 / 200000: 2.0865
Loss at iteration   80000 / 200000: 1.8507
Loss at iteration   90000 / 200000: 2.5262
Loss at iteration  100000 / 200000: 2.2370
Loss at iteration  110000 / 200000: 2.2191
Loss at iteration  120000 / 200000: 2.0212
Loss at iteration  130000 / 200000: 1.7962
Loss at iteration  140000 / 200000: 1.8132
Loss at iteration  150000 / 200000: 1.9622
Loss at iteration  160000 / 200000: 1.9555
Loss at iteration  170000 / 200000: 1.8946
Loss at iteration  180000 / 200000: 2.0191
Loss at iteration  190000 / 200000: 1.5637
2.301938533782959</code></pre>
</div>
</div>
<div id="fab38e30" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>plt.plot(np.log10(lossi))<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="character_level_ngram_model_v3_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="598d8b0e" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>() <span class="co"># this decorator disables gradient tracking</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We can also use a context manager (look this up)</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_split_loss(split):</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get the appropriate x an y based on the desired data split</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>    x,y <span class="op">=</span> {</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">"train"</span>: (xstr, ystr),</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"val"</span>:(xsdev, ysdev),</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"test"</span>: (xstest, ystest)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    }[split]</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    xemb <span class="op">=</span> C[x].view(<span class="op">-</span><span class="dv">1</span>,context_length<span class="op">*</span>embed_size)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#xembcat = xemb</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> xemb <span class="op">@</span> W1 <span class="co">#+ b1</span></span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">#hpreact = bngain*((hpreact - hpreact.mean(dim = 0, keepdim = True))/(hpreact.std(dim = 0, keepdim = True))) + bnbias</span></span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    hpreact <span class="op">=</span> bngain <span class="op">*</span> ((hpreact <span class="op">-</span> bnmean_running)<span class="op">/</span>bnstd_running) <span class="op">+</span> bnbias </span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    xh <span class="op">=</span> torch.tanh(hpreact)</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> xh <span class="op">@</span> W2 <span class="op">+</span> b2</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(logits, y)</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(split, loss.item())</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>calculate_split_loss(<span class="st">'train'</span>)</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>calculate_split_loss(<span class="st">'val'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>train 1.9572734832763672
val 1.9717477560043335</code></pre>
</div>
</div>
</section>
<section id="tracking-losses" class="level1">
<h1>Tracking losses</h1>
<p>Original loss - without any optimization Train - 2.024 Val - 2.044</p>
<p>Fix: logits (softmax being confidently wrong and assigning large probabilities to one character) Train - 1.942 Val - 1.957</p>
<p>Fix: tanh layer being too saturated at initialization Train - 1.938 Val - 1.950</p>
<p>Fix: scale W1 using Kaiming initialization factor Train - 1.928 Val - 1.942</p>
<p>Fix: Add simple batchnormalization. We dont expect batch normalization to do much here because its a shallow network and we have already scaled W1. We see that in this case it makes the losses worse. Train - 1.959 Val - 1.972 The above remain the same when we calculate running mean and running std for the batch norm layer. It also remains the same after removing the bias term in the linear layer before batch norm layer.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>