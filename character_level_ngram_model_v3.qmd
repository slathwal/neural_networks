---
title: Developing a character-prediction n-gram model using MLP - version 3
author: Shefali Lathwal
date: 2025-06-12
date-modified: last-modified
format: html
jupyter: cs224n
echo: true
toc: true
---

In this notebook, we will work on the following goals:

- Understand the activation steps within a neural network and the gradients that are flowing back through the network

- How to do effective initialization of weights

# Import dependencies
```{python}
import random
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
```

# Read the data file and create a mapping for all characters
```{python}
with open("data/names.txt") as file:
    words = file.read().splitlines()
file.close()

print("Total words in the data:",len(words))

# Create the lookup for character indices including a start and end of line character to the all_chars list

all_chars = sorted(list(set("".join(words)))+["*"])

stoi = {s:i for i, s in enumerate(all_chars)}
itos  = {i:s for s, i in stoi.items()}
print(f"{stoi=}\n{itos=}")
```

# Set some parameters
```{python}
vocab_size = len(all_chars) # vocabulary size
context_length = 3 # no of characters in context for prediction
embed_size = 10 # embedding vector length for each character
n_hidden = 200 # number of neurons in the hidden layer
min_batch_size = 32 # batch size in each gradient descent iteration

# Print parameters
print(f"{vocab_size=}\n{context_length=}\n{embed_size=}\n{n_hidden=}\n{min_batch_size=}")
```

# Build the training, validation and test datasets

```{python}
def build_dataset(words, context_length = context_length):
    xs, ys = [], []
    for word in words:
        chs = "*"*context_length+word+"*"
        #print(word)
        for ind in range(len(chs) - context_length):
            ch1 = chs[ind:ind+context_length]
            ch2 = chs[ind+context_length]
            #print(f"{ch1} ----> {ch2}")
            xs.append([stoi[ch] for ch in ch1])
            ys.append(stoi[ch2])

    xs = torch.tensor(xs) # 10X3
    ys = torch.tensor(ys) # 10
    print(xs.shape, ys.shape)
    return xs, ys

random.shuffle(words)
n1 = int(0.8*len(words))
n2 = int(0.9*len(words))
n1, n2

xstr, ystr = build_dataset(words[:n1])
xsdev, ysdev = build_dataset(words[n1:n2])
xstest, ystest = build_dataset(words[n2:])
```

# Initialize the neural network
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g)

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g)
b1 = torch.randn(size = (n_hidden,), generator=g)

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g)
b2 = torch.rand(size = (vocab_size,), generator=g)

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True

print("Number of parameters:", sum(p.nelement() for p in parameters))
```

# Train the neural network
```{python}
n_iters = 200000
lossi = []
learning_rate = 0.2
for i in range(n_iters): 
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, xstr.shape[0], (min_batch_size,), generator=g)
    Xb, Yb = xstr[ix], ystr[ix] # Batch X, Y 
    # forward pass
    xemb = C[Xb].view(min_batch_size, context_length*embed_size) # embed the characters into vectors and concatenate the vectors
    hpreact = (xemb @ W1 + b1) # hidden layer pre-activation
    h = torch.tanh(hpreact) # hidden layer
    logits = h @ W2 + b2 # output layer
    loss = F.cross_entropy(logits, Yb) # loss function

    if i%10000 == 0:
        print(f"Loss at iteration {i:7d} /{n_iters:7d}: {loss.item():.4f}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()

    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad

    # track statistics
    lossi.append(loss.item())
print(f"{loss.item()}")

#The above loss is just for the minibatch. Calculating loss for the full training data and validation data later.
```

```{python}
# It can be useful to plot logloss instead of loss
plt.figure()
plt.plot(np.arange(n_iters), np.log10(lossi))
plt.xlabel("iteration")
plt.ylabel("Log loss")
```

# Evaluation code 
- Calculate the loss for selected split of the data - train, dev, or test
```{python}
@torch.no_grad() # this decorator disables gradient tracking
# We can also use a context manager (look this up)
def calculate_split_loss(split):
    # get the appropriate x an y based on the desired data split
    x,y = {
        "train": (xstr, ystr),
        "val":(xsdev, ysdev),
        "test": (xstest, ystest)
    }[split]
    xemb = C[x].view(-1,context_length*embed_size)
    #xembcat = xemb
    xh = (xemb @ W1 + b1).tanh()
    logits = xh @ W2 + b2
    loss = F.cross_entropy(logits, y)
    print(split, loss.item())

calculate_split_loss('train')
calculate_split_loss('val')
```

# Sample from the model
```{python}
start_char = "*"
end_char = "*"
g = torch.Generator().manual_seed(123434)
start_ind = stoi[start_char]
for _ in range(20):
    context = [start_ind]*context_length
    all_chars = [start_char]*context_length
    while True:  
        # Now context contains the three character indices for our example
        xembp = C[torch.tensor(context)].view(1, -1) # Want to generate just one character at a time.
        xhp = (xembp @ W1 + b1).tanh() 
        logitsp = xhp @ W2 + b2 
        # Directly use the softmax function to calculate probabilites from logits
        probsp = F.softmax(logitsp, dim = 1)
        ind = torch.multinomial(probsp, num_samples = 1, replacement = True, generator = g)
        ch = itos[ind.item()]
        if (ch == end_char):
            all_chars.append(ch)
            break
        else:   
            all_chars.append(ch)
            context = context[1:]+[ind.item()]
    print("".join(all_chars[2:]))
```

# Expected loss at initialization

```{python}
-torch.tensor(1./vocab_size).log()
```

We expect a loss of about 3.4 at initialization, but right now we are getting a loss of about 27

If our logits are big numbers, we start to get very high losses. Therefore, at initialization, we want logits to either be equal for all output possibilities or close to and distributed around zero and not arbitrarily high numbers.

# Inspect and play with the logits and other intermediate values at initialization

Re-initialize the neural network
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g)

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) * (5/3)/((context_length*embed_size)**0.5) #0.2
b1 = torch.randn(size = (n_hidden,), generator=g) * 0.01

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) * 0.01
b2 = torch.rand(size = (vocab_size,), generator=g) * 0

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True

print("Number of parameters:", sum(p.nelement() for p in parameters))
```

Run the first forward step
```{python}
n_iters = 200000
lossi = []
learning_rate = 0.2
for i in range(n_iters): 
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, xstr.shape[0], (min_batch_size,), generator=g)
    Xb, Yb = xstr[ix], ystr[ix] # Batch X, Y 
    # forward pass
    xemb = C[Xb].view(min_batch_size, context_length*embed_size) # embed the characters into vectors and concatenate the vectors
    hpreact = (xemb @ W1 + b1) # hidden layer pre-activation
    h = torch.tanh(hpreact) # hidden layer
    logits = h @ W2 + b2 # output layer
    loss = F.cross_entropy(logits, Yb) # loss function

    if i%10000 == 0:
        print(f"Loss at iteration {i:7d} /{n_iters:7d}: {loss.item():.4f}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()

    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad

    # track statistics
    lossi.append(loss.item())
    #break

print(f"{loss.item()}")
```

we can look at the output coming out of the hidden layer as an image. In the image, white represents true and black represents false. There shouldn't be too much white in this.

```{python}
plt.figure(figsize = (20,10))
plt.imshow(h.abs() > 0.99, cmap = 'gray', interpolation = 'nearest')
```

```{python}
plt.plot(np.log10(lossi))
```

```{python}
calculate_split_loss('train')
calculate_split_loss('val')
```

::: {.callout-note}
I am re-running cells in the notebook, so the output of the cells below will not show the logits and intermediate values obtained without any modification of parameter initialization.
:::
Look at logit values
```{python}
logits[0]
```

Look at values of h 

```{python}
h.shape # h is 32Xn_hidden because size of mini-batch is 32
h.view(-1).tolist() # Create a list
plt.figure()
plt.title("values coming out of the hidden layer")
plt.hist(h.view(-1).tolist(), bins = 50);


plt.figure()
plt.hist(hpreact.view(-1).tolist(), bins = 50)
plt.title("value of preactivations feeding into the tanh function")
```

Logits:
- If we look at logits from random uniform initialization, we'll see that the numbers are very high, as much as 18. Since logit calculation depends on W2 and b2, we can fix this by reducing W2 and b2.

- If I scale down W2 by 0.01 and make b2=0, I get an initial loss of about 3.43, which is close to what we expect from uniform distribution of probabiltiies

- We don't want weights of a neural network layer exactly to be zero because ...

- Just by initializing the logits to be small, we can get the training loss and validation loss down from 2.024 and 2.044 to 1.942 and 1.957

h - output of hidden layer:
- If we look at values of h, we will see that too many values are in the flat regions of tanh, i.e., -1 or +1. In these regions, gradients are approximately zero and therefore will not affect parameters much.
- The values coming from tanh are in the +1 and -1 region, because the preactivations feeding into the tanh function take on very large positive and negative values.
- if for any neuron, all the inputs turn out to be in the flat region of tanh, it would never learn in that batch and it would be a dead neuron, because all gradients would be zero.
- We can reduce the values of preactivation by reducing W1 and b1
- Fixing the above reduces loss a little bit as well to 1.938 for train and 1.950 for val.
- We are improving on losses because due to better initialization, we spend more cycles actually optimizing than fixing initialization issues.

- With deeper networks - example 50 layers - these problems stack up and deeper networks become less forgiving to problems like initialization.

# Tracking losses

Original loss - without any optimization
Train - 2.024
Val - 2.044

Fix logits (softmax being confidently wrong and assigning large probabilities to one character)
Train - 1.942
Val - 1.957

Fix tanh layer being too saturated at initialization
Train - 1.938
Val - 1.950

Fix: scale W1 using Kaiming initialization factor
Train - 1.928
Val - 1.942


# How to set scales for layer weights that help with less tanh saturation and smaller logit values at initialization
Visualize the problem first.

Look at statistics of gaussian distributions at initialization and when they go through computations like matrix multiplication

```{python}
# Generate random numbers first
x = torch.randn(1000, 10)
W = torch.randn(10, 200)
y = x @ W
print(x.mean(), x.std())
print(W.mean(), W.std())
print(y.mean(), y.std())
fig, ax = plt.subplots(2, 1, sharex = True)
ax[0].hist(x.view(-1).tolist(), 50, density = True);
ax[0].set_title("Initialization from normal distribution");
ax[1].hist(y.view(-1).tolist(), 50, density = True);
ax[1].set_title("Distribution after matrix multiplication of two parameters\n initialized independently from normal distribution");
fig.tight_layout();
```

We see that after matrix multiplication, the standard deviation of the resulting distributin increases.
Thereforem the question is how do we scale W to preserve standard deviation to be 1.

Mathematically, to preserve the standard deviation after matrix multiplication, we divide by the square root of `fan_in`, which is the number of input elements to the next layer

```{python}
x = torch.randn(1000, 10)
W = torch.randn(10, 200) / 10**0.5
y = x @ W
print(x.mean(), x.std())
print(W.mean(), W.std())
print(y.mean(), y.std())
fig, ax = plt.subplots(2, 1, sharex = True)
ax[0].hist(x.view(-1).tolist(), 50, density = True);
ax[0].set_title("Initialization from normal distribution");
ax[1].hist(y.view(-1).tolist(), 50, density = True);
ax[1].set_title("Distribution after matrix multiplication of two parameters\n initialized independently from normal distribution \n after scaling the weights");
fig.tight_layout();
```

A paper that explains these scaling factors to maintain good distributions throughout a neural network during initialization is Kaiming He et al. 2015. They studied convolutional neural networs and they studied ReLU and PReLU non-linearities. In their analysis of forward activation of the neural net, they find that instead of reducing weights by `sqrt(fan_in)`, they also have to multiply by a `gain` to account for half of the numbers being discarded in ReLU.

Kaiming normalization is also implemented in pytorch in `torch.nn.init.kaiming_normal`. The documentation page for this function provides the gains for different non-linearities.

Intuitively, we need a gain because non-linearities like tanh or ReLU are squasing functions that narrow the distribution. Therefore, we need a small gain to counter the squashing.

There are modern techniques that counter some of the issues with incorrect normalizations:
- residul connections
- normalization layers like batch normalizations, layer normalizations etc.
- better optimizers like Adam.

In practice, scale the std of weights by `gain/sqrt(fan_in)`, which means multiplying normally initialized weights by the factor above.

# Batch normalization

- Mititgates some of the issues associated with incorrect initialization.

Initialize the neural network
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g)

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) * (5/3)/((context_length*embed_size)**0.5) #0.2
b1 = torch.randn(size = (n_hidden,), generator=g) * 0.01

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) * 0.01
b2 = torch.rand(size = (vocab_size,), generator=g) * 0

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True

print("Number of parameters:", sum(p.nelement() for p in parameters))
```

Training loop:
```{python}
n_iters = 200000
lossi = []
learning_rate = 0.2
for i in range(n_iters): 
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, xstr.shape[0], (min_batch_size,), generator=g)
    Xb, Yb = xstr[ix], ystr[ix] # Batch X, Y 
    # forward pass
    xemb = C[Xb].view(min_batch_size, context_length*embed_size) # embed the characters into vectors and concatenate the vectors
    hpreact = (xemb @ W1 + b1) # hidden layer pre-activation
    h = torch.tanh(hpreact) # hidden layer
    logits = h @ W2 + b2 # output layer
    loss = F.cross_entropy(logits, Yb) # loss function

    if i%10000 == 0:
        print(f"Loss at iteration {i:7d} /{n_iters:7d}: {loss.item():.4f}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()

    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad

    # track statistics
    lossi.append(loss.item())
    #break

print(f"{loss.item()}")
```

```{python}
plt.plot(np.log10(lossi))
```

```{python}
calculate_split_loss('train')
calculate_split_loss('val')
```




