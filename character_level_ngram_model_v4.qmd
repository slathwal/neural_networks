---
title: Developing a character-prediction n-gram model using MLP - version 4
author: Shefali Lathwal
date: 2025-06-13
date-modified: last-modified
format: html
jupyter: cs224n
echo: true
toc: true
---
# Learning goals
In this notebook, we will implement the ideas from version 3 - using a MLP as defined in Bengio et al. 2003 paper, but with the following modifications:
- We will pytorch-ify the code that we wrote in the previous notebook, which means we will write classes as available in pytorch and use them to build our neural network. We will then compare our code with the classes directly available from pytorch.
- We will use ideas developed previously to normalize the statistics of all parameters in the neural network at initialization and during training so that our neural networks are well-behaved. Namely, we will use Kaiming initialization and BatchNorm.
- We will visualize activation statistics using some graphs.

# Import dependencies
```{python}
import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
```

# Write the layer modules
```{python}
class Linear:

    def __init__(self, fan_in, fan_out, bias = True):
        self. weight = torch.randn(fan_in, fan_out) * (fan_in)**0.5 # kaiming initialization for linear layer
        self.bias = torch.zeros(fan_out) if bias else None
    
    def __repr__(self):
        if self.bias is not None:
            return (f"Linear Module with weight {self.weight.shape} and bias {self.bias.shape}")
        else:
            return (f"Linear Module with weight tensor of shape {self.weight.shape} and no bias")

    def __call__(self, x):
        assert (x.shape[1] == self.weight.shape[0]), "Tensor dimensions of input and weight do not match"
        self.out = x @ self.weight
        if self.out is not None:
            self.out += self.bias
        return self.out
    
    def parameters(self):
        return [self.weight] + ([self.bias] if self.bias is not None else [])


class Tanh:
    #def __init__(self): # Do not define __init__ here otherwise there is a strange behaviour where the __call__ function does not work properly.
    #    pass
    def __call__(self, x):
        self.out = torch.tanh(x)
        return self.out
    
    def parameters(self):
        return []

class BatchNorm1d:

    def __init__(self, dim, eps=1e-5, momentum = 0.01):
        self.eps = eps
        self.momentum = momentum
        self.training = True # Training mode on by default
        # parameters trained by backpropagation
        self.gamma = torch.ones(dim)
        self.beta = torch.zeros(dim)
        # buffers
        self.running_mean = torch.zeros(dim)
        self.running_var = torch.ones(dim)
    
    def __call__(self, x):
        # calculate the forward pass
        if self.training:
            xmean = x.mean(0, keepdim = True)
            xvar = x.var(0, keepdim = True)
        else:
            xmean = self.running_mean
            xvar = self.running_var
        xhat = (x - xmean)/torch.sqrt(xvar + self.eps)
        self.out = self.gamma * xhat + self.beta

        # update buffers when in training mode
        if self.training:
            with torch.no_grad():
                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean
                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar
        return self.out
    
    def parameters(self):
        return [self.gamma, self.beta]
```

# Making sure that classes are functioning correctly
```{python}
layer = Linear(10, 100)
x = torch.randn(1,10)
x
layer(x)
layer.out
output = layer(x)
output
layer.out
layer(x)
```

```{python}
layer = Tanh()
linear_o = layer(x)
linear_o
#layer.parameters()
layer(x)
```

```{python}
layer = BatchNorm1d(dim = 15) # dim = number of neurons in the layer. a mean and variance will be calculated for each neuron by taking values from the whole mini-batch
layer.__class__
x = torch.randn(10, 15) # The second dimension here must match the dim parameters of the BatchNorm1d layer.
x.shape
batch_norm_output = layer(x)
print(x.mean(dim = 0, keepdim = True), layer.running_mean, sep = "\n")
print(x.var(dim = 0, keepdim=True), layer.running_var, sep = "\n")
len(layer.parameters()), sum(p.nelement() for p in layer.parameters())
```


# Build MLP using the above modules



# The statistics to monitor at initialization
---------------------------------------
TOEDIT
- distribution of activations in the forward pass
- distribution of saturated values in each non-linear layer in the forward pass

- distribution of gradient/data ratio in each layer during backward pass as well as mean and std of gradients
- distribution of ratio of update value with the parameter value at each iteration.

