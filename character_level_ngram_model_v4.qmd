---
title: Developing a character-prediction n-gram model using MLP - version 4
author: Shefali Lathwal
date: 2025-06-13
date-modified: last-modified
format: html
jupyter: cs224n
echo: true
toc: true
---
# Learning goals
In this notebook, we will implement the ideas from version 3 - using a MLP as defined in Bengio et al. 2003 paper, but with the following modifications:
- We will pytorch-ify the code that we wrote in the previous notebook, which means we will write classes as available in pytorch and use them to build our neural network. We will then compare our code with the classes directly available from pytorch.
- We will use ideas developed previously to normalize the statistics of all parameters in the neural network at initialization and during training so that our neural networks are well-behaved. Namely, we will use Kaiming initialization and BatchNorm.
- We will visualize activation statistics using some graphs.

# Import dependencies
```{python}
import torch
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F
import random
```

# Write the layer modules
```{python}
class Linear:

    def __init__(self, fan_in, fan_out, bias = True):
        self. weight = torch.randn((fan_in, fan_out), generator = g) / (fan_in)**0.5 # kaiming initialization for linear layer
        self.bias = torch.zeros(fan_out) if bias else None
    
    def __repr__(self):
        if self.bias is not None:
            return (f"Linear Module with weight {self.weight.shape} and bias {self.bias.shape}")
        else:
            return (f"Linear Module with weight tensor of shape {self.weight.shape} and no bias")

    def __call__(self, x):
        assert (x.shape[1] == self.weight.shape[0]), "Tensor dimensions of input and weight do not match"
        self.out = x @ self.weight
        if self.bias is not None:
            self.out += self.bias
        return self.out
    
    def parameters(self):
        return [self.weight] + ([self.bias] if self.bias is not None else [])


class Tanh:
    #def __init__(self): # Do not define __init__ here otherwise there is a strange behaviour where the __call__ function does not work properly.
    #    pass
    def __call__(self, x):
        self.out = torch.tanh(x)
        return self.out
    
    def parameters(self):
        return []

class BatchNorm1d:

    def __init__(self, dim, eps=1e-5, momentum = 0.01):
        self.eps = eps
        self.momentum = momentum
        self.training = True # Training mode on by default
        # parameters trained by backpropagation
        self.gamma = torch.ones(dim)
        self.beta = torch.zeros(dim)
        # buffers
        self.running_mean = torch.zeros(dim)
        self.running_var = torch.ones(dim)
    
    def __call__(self, x):
        # calculate the forward pass
        if self.training:
            xmean = x.mean(0, keepdim = True)
            xvar = x.var(0, keepdim = True)
        else:
            xmean = self.running_mean
            xvar = self.running_var
        xhat = (x - xmean)/torch.sqrt(xvar + self.eps)
        self.out = self.gamma * xhat + self.beta

        # update buffers when in training mode
        if self.training:
            with torch.no_grad():
                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean
                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar
        return self.out
    
    def parameters(self):
        return [self.gamma, self.beta]
```

# Making sure that classes are functioning correctly
```{python}
layer = Linear(10, 100)
x = torch.randn(1,10)
x
layer(x)
layer.out
output = layer(x)
output
layer.out
layer(x)
```

```{python}
layer = Tanh()
linear_o = layer(x)
linear_o
#layer.parameters()
layer(x)
```

```{python}
layer = BatchNorm1d(dim = 15) # dim = number of neurons in the layer. a mean and variance will be calculated for each neuron by taking values from the whole mini-batch
layer.__class__
x = torch.randn(10, 15) # The second dimension here must match the dim parameters of the BatchNorm1d layer.
x.shape
batch_norm_output = layer(x)
print(x.mean(dim = 0, keepdim = True), layer.running_mean, sep = "\n")
print(x.var(dim = 0, keepdim=True), layer.running_var, sep = "\n")
len(layer.parameters()), sum(p.nelement() for p in layer.parameters())
```

# read the words, define the vocabulary and build mapping from vocabulary to integers
```{python}
with open("data/names.txt") as file:
    words = file.read().splitlines()
len(words)

# Build a character vocabulary
all_chars = sorted(list(set(''.join(words)+"*")))

# Build a mapping dictionary
stoi = {s:i for i,s in enumerate(all_chars)}
itos = {i:s for s,i in stoi.items()}
```


# Build MLP using the above modules

```{python}
n_hidden = 100
embed_size = 10
vocab_size = len(all_chars)
mini_batch_size = 32
context_length = 3
```

# Build the training data

```{python}
def build_dataset(words):
    xs = []
    ys = []
    for word in words:
        context = ['*']*context_length
        chs = word+'*'
        #ch_list = [ch for ch in chs]
        #print(ch_list)
        for ch in chs:
            #print(ch)
            #print(''.join(context), ch)
            xs.append([stoi[s] for s in context])
            ys.append(stoi[ch])
            context = context[1:]+[ch]
    xs = torch.tensor(xs)
    ys = torch.tensor(ys)
    return xs, ys

random.shuffle(words)
n1 = int(0.8*len(words))
n2 = int(0.9*len(words))
n1, n2

xstr, ystr = build_dataset(words[:n1])
xsdev, ysdev = build_dataset(words[n1:n2])
xstest, ystest = build_dataset(words[n2:])
```

```{python}
ystr[:10], xstr[:10]
```

# Build the neural network

## Intialize
```{python}
g = torch.Generator().manual_seed(624896294)
# Initialize the embedding layer
C = torch.randn((vocab_size, embed_size), generator = g)
layers = [
    Linear(context_length*embed_size, n_hidden), Tanh(),
    Linear(n_hidden, n_hidden), Tanh(),
    Linear(n_hidden, n_hidden), Tanh(),
    Linear(n_hidden, n_hidden), Tanh(),
    Linear(n_hidden, n_hidden), Tanh(),
    Linear(n_hidden, vocab_size),
]

# Perform proper initialization
with torch.no_grad():
    # make last layer less confident
    layers[-1].weight *= 0.1
    # Apply gain to all other layer weights at initialization
    for layer in layers[:-1]:
        if isinstance(layer, Linear):
            layer.weight *= 5/3

# Collect all the parameters
parameters = [C] + [p for layer in layers for p in layer.parameters()]
print(sum(p.nelement() for p in parameters)) # total number of parameters in the network

for p in parameters:
    p.requires_grad = True

# for layer in layers[0:1]:
#     print(layer.weight)
```

## Train the network
```{python}
max_steps = 200000
lossi = []
ud = []

for i in range(max_steps):
    # sample indices from minibatch
    ix = torch.randint(0,xstr.shape[0], (mini_batch_size,), generator = g)
    Xb, Yb = xstr[ix], ystr[ix]

    # forward pass
    emb = C[Xb] # embed characters into vectors
    x = C[Xb].view(emb.shape[0], context_length*embed_size) # concatenate the vecotrs
    #print(x)
    for layer in layers:
        x = layer(x)
        #print(x)
    loss = F.cross_entropy(x, Yb) # loss function

    # backward pass
    for layer in layers:
        layer.out.retain_grad() # required for plotting the gradient of the layer output later on. Not required for actual training

    for p in parameters:
        p.grad = None
    loss.backward()

    # update parameters
    lr = 0.2
    if i < 10000:
        lr = 0.2 
    elif 10000 <= i < 50000:
        lr = lr/10
    else:
        lr = lr/100 # step learning rate decay
    for p in parameters:
        p.data += -lr * p.grad

    # track statistics
    if i % 10000 == 0:
        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')
    lossi.append(loss.log10().item())
    with torch.no_grad():
        ud.append([(lr*p.grad.std() / p.data.std()).log10().item() for p in parameters])
    if i > 20000:
        break
```

My initial loss is very high, despite using Kaiming initialization. So, I will use some graphs to visualize the statistics at initialization. I discovered a big where instead of dividing by a factor of `sqrt(fan_in)`, I was multiplying by that number. Therefore, all my tanh outputs were saturated.


# Statistics to monitor at initialization

## Look at the % saturation of the non-linear layers
```{python}

plt.figure()
legends = []

for i, layer in enumerate(layers[:-1]):
    if isinstance(layer, Tanh):
        t = layer.out
        #print(t.shape)
        print(f'layer {i} ({layer.__class__.__name__}): mean: {t.mean():.2f}, std: {t.std():.2f}, saturated: {(t.abs()>0.97).float().mean()*100:.2f}')
        hy, hx = torch.histogram(t, density = True)
        plt.plot(hx[:-1].detach(), hy.detach())
        legends.append(f'layers {i} ({layer.__class__.__name__})')
plt.legend(legends)
plt.title('activation distribution')
```

## Look at gradient values of the non-linear layers
We are looking to see if there is a nice distribution of gradients and that all gradients shouldn't be squashed or too spread out.
```{python}
plt.figure()
legends = []

for i, layer in enumerate(layers[:-1]):
    if isinstance(layer, Tanh):
        t = layer.out.grad
        #print(t)
        #print(t.shape)
        print(f'layer {i} ({layer.__class__.__name__}): mean: {t.mean():+f}, std: {t.std():e}')
        hy, hx = torch.histogram(t, density = True)
        plt.plot(hx[:-1].detach(), hy.detach()) # detach function takes a value out of computational graph
        legends.append(f'layers {i} ({layer.__class__.__name__})')
plt.legend(legends)
plt.title('layer gradient distribution')
```

## Look at the ratio of gradient to data for each parameter
```{python}
plt.figure()
legends = []
for i,p in enumerate(parameters):
    t = p.grad
    if p.ndim == 2: # Only look at weights parameters which are 2-D, and not biases- 
        print(f'weight {p.shape}, mean {t.mean():+f}, std {t.std():e}, grad:data ratio {t.std()/p.std():e}')
        hy, hx = torch.histogram(t, density = True)
        plt.plot(hx[:-1].detach(), hy.detach()) # detach function takes a value out of computational graph
        legends.append(f'{i} ({p.shape})')
plt.legend(legends)
plt.title('weights gradient distribution')
```

## Look at the update to the data ratio
```{python}
plt.figure()
legends = []
for i,p in enumerate(parameters):
    t = p.grad
    if p.ndim == 2: # Only look at weights parameters which are 2-D, and not biases- 
        plt.plot([ud[j][i] for j in range(len(ud))])
        legends.append(f'param {i:d}')
plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3
plt.legend(legends)
```

- If we have no tanh layer, the correct gain for weights of each layer would be 1.

- If gradients are too large compared to the data, then we would have a problem because we are updating the data with some fraction of the gradient.

- If weights at initialization are high in one layer compared to other layers, then we would be training that layer faster compared to other layers, because it's gradients would also be larger.

- The ratio of update to data should not be too much above 1e-3. If it's below 1e-3, it means our learning rate is too low. So for 10,000 iterations, our learning rate is too low.

To summarize:
At initialization, you want all distributions of gradients and activations throughout all layers of neural network to be roughly Gaussian.
- distribution of activations in the forward pass and % saturated values output from each non-linear layer.
- distributions of the gradients flowing back through each non-linear layer.
- distribution of gradient/data ratio in each layer during backward pass as well as mean and std of gradients
- distribution of ratio of update value with the parameter value at each iteration.

# let's introduce the BatchNorm layer in the neural network

## Initialization

```{python}
g = torch.Generator().manual_seed(624896294)
# Initialize the embedding layer
C = torch.randn((vocab_size, embed_size), generator = g)
layers = [
    Linear(context_length*embed_size, n_hidden, bias = False), BatchNorm1d(n_hidden), Tanh(),
    Linear(n_hidden, n_hidden, bias = False), BatchNorm1d(n_hidden), Tanh(),
    Linear(n_hidden, n_hidden, bias = False), BatchNorm1d(n_hidden), Tanh(),
    Linear(n_hidden, n_hidden, bias = False), BatchNorm1d(n_hidden), Tanh(),
    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),
    Linear(n_hidden, vocab_size, bias = False), BatchNorm1d(vocab_size),
]

# Perform proper initialization
with torch.no_grad():
    # make last layer less confident
    # when we are using batchnorm, we would change gamma instead of weight.
    layers[-1].gamma *= 0.1
    #layers[-1].weight *= 0.1
    # Apply gain to all other layer weights at initialization
    for layer in layers[:-1]:
        if isinstance(layer, Linear):
            layer.weight *= 5/3

# Collect all the parameters
parameters = [C] + [p for layer in layers for p in layer.parameters()]
print(sum(p.nelement() for p in parameters)) # total number of parameters in the network

for p in parameters:
    p.requires_grad = True

```

## Train the network
```{python}
max_steps = 200000
lossi = []
ud = []

for i in range(max_steps):
    # sample indices from minibatch
    ix = torch.randint(0,xstr.shape[0], (mini_batch_size,), generator = g)
    Xb, Yb = xstr[ix], ystr[ix]

    # forward pass
    emb = C[Xb] # embed characters into vectors
    x = C[Xb].view(emb.shape[0], context_length*embed_size) # concatenate the vecotrs
    #print(x)
    for layer in layers:
        x = layer(x)
        #print(x)
    loss = F.cross_entropy(x, Yb) # loss function

    # backward pass
    for layer in layers:
        layer.out.retain_grad() # required for plotting the gradient of the layer output later on. Not required for actual training

    for p in parameters:
        p.grad = None
    loss.backward()

    # update parameters
    lr = 0.2
    if i < 10000:
        lr = 0.2 
    elif 10000 <= i < 50000:
        lr = lr/10
    else:
        lr = lr/100 # step learning rate decay
    for p in parameters:
        p.data += -lr * p.grad

    # track statistics
    if i % 10000 == 0:
        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')
    lossi.append(loss.log10().item())
    #with torch.no_grad():
    #    ud.append([(lr*p.grad.std() / p.data.std()).log10().item() for p in parameters])
    
```

The above neural network is much less sensitive to gain values. Therefore, if we change the gain, the activation and gradient statistics will remain well-behaved, but the update to data ratio will change and we may have to re-tune the learning rate. If we decrease the gain, some layers train faster. if we increase the gain, layers train slower. Think about why that is based on backpropagation through the batch-norm layer.

```{python}
plt.plot(lossi)
```

# Evaluate the trained model

- Calculate the loss for selected split of the data - train, dev, or test
```{python}
@torch.no_grad() # this decorator disables gradient tracking
# We can also use a context manager (look this up)
def calculate_split_loss(split):
    # get the appropriate x an y based on the desired data split
    x,y = {
        "train": (xstr, ystr),
        "val":(xsdev, ysdev),
        "test": (xstest, ystest)
    }[split]
    x = C[x].view(-1,context_length*embed_size)
    #xembcat = xemb
    for layer in layers:
        if isinstance(layer, BatchNorm1d):
            layer.training = False
        x = layer(x)
    loss = F.cross_entropy(x, y)
    print(split, loss.item())

calculate_split_loss('train')
calculate_split_loss('val')
```


# Make predictions
```{python}
start_char = "*"
end_char = "*"
g = torch.Generator().manual_seed(123434)
start_ind = stoi[start_char]
for _ in range(20):
    context = [start_ind]*context_length
    all_chars = [start_char]*context_length
    while True:  
        # Now context contains the three character indices for our example
        xp = C[torch.tensor(context)].view(1, -1) # Want to generate just one character at a time.
        #print(xembp.shape)
        for layer in layers:
            if isinstance(layer, BatchNorm1d):
                layer.training = False
            xp = layer(xp)
            #print(x.shape)
        # Directly use the softmax function to calculate probabilites from logits
        probsp = F.softmax(xp, dim = 1)
        ind = torch.multinomial(probsp, num_samples = 1, replacement = True, generator = g)
        ch = itos[ind.item()]
        if (ch == end_char):
            all_chars.append(ch)
            break
        else:   
            all_chars.append(ch)
            context = context[1:]+[ind.item()]
    print("".join(all_chars))
```