---
title: "Learning to build NLP models using deep-learning"
author: "Shefali Lathwal"
date: 2025-06-07
date-modified: last-modified
format: html
echo: true
toc: true
jupyter: cs224n
---

Following along lectures from CS224n and Andrei Karpathy's Neural Networks: zero-to-hero series.

- Start with a character level language model

- Given a few characters, predict the next character in the sequence.

- We will start with character-level models and eventually go to word-level models.

- Start with bigram models that just predict a character given a single character
    - We should pay attention to characters that start and end words as well.

- Also think about torch summing operations

- Also think about sampling from multinomial distributions in torch

- Think about broadcasting rules/broadcasting semantics in torch
    - Look at tutorials for broadcasting and practice it

Step 1: Train a bigram character level model based on frequencies of character occurences in a training dataset.

Step 2: Train a neural network to predict the next character based on a training dataset.

# Get some data

Get some Indian names from a [website](https://www.acko.com/health-insurance/s/pregnancy/baby-names/modern-indian-baby-names-for-boys-and-girls-with-meanings/) and save them in a file called `names.txt`

```{python}
import re

with open("names.txt") as file:
    text = file.read().splitlines()
file.close()
# Remove empty strings from the list
text = [x for x in text if x]
# Remove the characters after '-'' as they are not part of the name and make all characters lowercase, and remove leading and training whitespaces
words = [word.split("-")[0].lower().strip() for word in text]
words[:5], len(words), min(len(w) for w in words), max(len(w) for w in words)
```

# Get a list of character bigrams from the word list
```{python}
bigrams={}
for word in words:
    chs = '.'+word+'.'
    for ch1, ch2 in zip(chs, chs[1:]):
        key = ch1+ch2
        #print(key)
        bigrams[key] = bigrams.get(key,0)+1
        #print(bigrams)
        #print(ch1, ch2, sep = "")
#bigrams
```

```{python}
sorted(bigrams.items(), key = lambda kv: kv[1], reverse = True)[:10]
```

# Save the bigrams into a tensor so that we can later use pytorch
Since tensor has no labels, we will need a mapping from indices to characters and vice-versa

```{python}
all_chars = list(set(char for word in words for char in word))
# ANother way to do the above
all_chars = list(set("".join(words)))
#print(all_chars)
len(all_chars)
# Character x is missing and so is the end character "."
all_chars.append("x")
all_chars = sorted(all_chars)
all_chars, len(all_chars)

# stoi = {}
stoi = {s:i for i,s in enumerate(all_chars)}
stoi['.'] = 26
itos = {i:s for s,i in stoi.items()}
stoi, itos
```

Now, initialize an empty tensor to save all the bigram counts

```{python}
import torch

# starting with a baseline of one to avoid zero counts.
N = torch.ones((27,27), dtype = torch.int32) # This is called model smoothing
N.shape

for word in words:
    chs = '.'+word+'.'
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1, ix2 = stoi[ch1], stoi[ch2]
        N[ix1, ix2] += 1

#N
```


# Visualize the matrix
```{python}
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize = (10, 10))
im = ax.imshow(N)
ax.set_xticks(range(len(all_chars)+1), labels = all_chars+["."])
ax.set_yticks(range(len(all_chars)+1), labels = all_chars+["."])

# Loop over data dimensions and create text annotations
for i in range(27):
    for j in range(27):
        text = ax.text(j, i, N[i, j].item(), ha = "center", va = "top", color = "white")
        text = ax.text(j, i, itos[i]+itos[j], ha = "center", va = "bottom", color = "white")
fig.tight_layout()
```

# Calculate probability of a character following abother character

```{python}
character = 'a'
idx1 = stoi['a']
counts = N[idx1].float()
sum_counts = sum(counts)
#p = N[idx1]/sum(N[idx1])
counts, sum_counts
p = counts/sum_counts
p
```

# Calculate the tensor of probabilities
We want to divide the original counts with sums of each row. We have to follow broadcasting semantics of tensors and have to understand what operations are happening in the background.
```{python}
Np = N.float()
# The dimension along which sum is computed is reduced to size 1 in output tensor
# I want to sum across all rows. Therefore, I want sum along all rows in a single column
rowsum_tensor = Np.sum(dim = 1, keepdim=True)
rowsum_tensor.shape, rowsum_tensor
# One row has a count of zero. Therefore, we will add a count of 1 to any value that has a value of zero, to avoid a final division by zero
Np = (Np/rowsum_tensor)

fig, ax = plt.subplots(figsize = (10, 10))
im = ax.imshow(N)
ax.set_xticks(range(len(all_chars)+1), labels = all_chars+["."])
ax.set_yticks(range(len(all_chars)+1), labels = all_chars+["."])

# Loop over data dimensions and create text annotations
for i in range(27):
    for j in range(27):
        text = ax.text(j, i, round(Np[i, j].item(),2), ha = "center", va = "top", color = "white")
        text = ax.text(j, i, itos[i]+itos[j], ha = "center", va = "bottom", color = "white")
fig.tight_layout()
```

# Make predictions based on calculated probabilities
We will make predictions by sampling from the calculated probabilities using `torch.multinomial`

Let's test out the random number generator first to get a feel of the function

```{python}
g = torch.Generator().manual_seed(347291378491) # fix the seed
seq = torch.rand(3, generator = g) # generate three random numbers from uniform distribution between 0 and 1
# Normalize the numbers to get a probability distribution
seq = seq/sum(seq)
print(seq)

torch.multinomial(seq, 10, replacement=True, generator=g) # We should get index=0, i.e., the first number 50% of the times, and numbers at index 1 and 2 approximately 25% and 21% of the time, respectively.
```

```{python}
gen = torch.Generator().manual_seed(347291378491)

for _ in range(20):
    char_list= ["."]
    ind = stoi["."]
    while True:
        p = Np[ind]
        # If all characters were equally likely
        #p = torch.ones(27)/27.0
        ind = torch.multinomial(input = p, num_samples = 1, replacement = True, generator=gen).item()
        #print(ind)
        #print(sample_ind)
        sample_char = itos[ind]
        char_list.append(sample_char)
        if ind == stoi["."]:
            break
    print(''.join(char_list))
```

# Evaluate the quality of the bigram model using a single number - loss

- For 27 characters/tokesn, if everything was equally likely, we would expect probability of each bigram to be `{python} f'{1./27:.4f}'`. Anything above 4% means that we have learned something from the data.

- If the model is trained well, then for the bigrams in the training data, the probability should be close to 1.

- For probabilties, we use a quantity called likelihood, which is a product of all probabilities. When we have a good model, the product of these probabilities should be very high. In practice, we use log-likelihood, which is a sum of log of all probabilities.

- Since log is a monotonic function, then if likelihood is high, log-likelihood should be high and negative of log-likelihood should be low.

- Therefore, we can use negative log-likelihood as the loss function. For a very good model, negative log-likelihood will be low.

- Typically we use an average of log-likelihood instead of raw log-likelihood, which can go from 0 to infinity.

```{python}
log_likelihood = 0.0
n = 0
for word in words:
    chs = '.'+word+'.'
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1, ix2 = stoi[ch1], stoi[ch2]
        # look at probability that bigram assigns to each bigram
        prob = Np[ix1,ix2]
        logprob = torch.log(prob)
        log_likelihood += logprob
        n += 1
        #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')
print(f'{log_likelihood=}')
nll = -log_likelihood/n
print(f'Average neg log likelihood for character-level bigram model\n {nll.item()=:.4f}')
```

using the above method, we can calculate the loss function for any given word.

```{python}
log_likelihood = 0.0
n = 0
words_list = ["aadhvita"]
for word in words_list:
    chs = '.'+word+'.'
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1, ix2 = stoi[ch1], stoi[ch2]
        # look at probability that bigram assigns to each bigram
        prob = Np[ix1,ix2]
        logprob = torch.log(prob)
        log_likelihood += logprob
        n += 1
        print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')
print(f'{log_likelihood=}')
nll = -log_likelihood/n
print(f'Negative log likelihood:\n {nll.item()=:.4f}')
```

# Building a character-prediction bigram model using pytorch

# Prepare the training data
Get all the bigrams in the data. Start with just one name to get a feel of what is happening in the model.

- Our training data will consist of xs that will have the index of the first character in the bigram
- Our ys (or labels) will consist of the index of the second character in the bigram

Process:
Our model will take in the first character stored in xs and predict the second character stored in ys. We will then calculate a loss function, which will be negative log-likelihood value. Once loss is calculated, we will perform back propagation to calculate gradients with respect to parameters in the model, then update the parameters based on the calculcated gradients. We will then do a forward pass again and re-calculate the loss and iterate.

```{python}
xs, ys = [], []

for word in words[:1]:
    chrs = "."+word+"."
    for ch1, ch2 in zip(chrs, chrs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        print(ch1, ch2, sep = "")
        xs.append(ix1)
        ys.append(ix2)


# Convert xs and ys to pytorch tensors
xs = torch.tensor(xs)
ys = torch.tensor(ys)
```

```{python}
print(type(xs[1]), type(ys[1]))
xs.dtype, ys.dtype, xs.shape, ys.shape
```

Our training data using the first word in our `names.txt` file consists of 9 bigram examples. 

xs and ys are a list of integers. But we cannot feed integers to a neural network.

- We will use one-hot encoding on the integers
- After one-hot encoding, our training data consists of 9 examples, each with 27 attributes.

```{python}
import torch.nn.functional as F
xenc = F.one_hot(xs, num_classes=27)
xenc, xenc.shape, plt.imshow(xenc),(xenc.dtype)

```

When we plug in values into a neural network, we want them to be floating points and not integers, but right now, the datatype ox `xenc` is `int64`. So we will need to convert it to float.
```{python}
xenc = xenc.float()
xenc.dtype, xenc.shape
```

# Define weights for each of the training examples

We are building a simple neural net that only has one layer that performs the transformation W.x Therefore, we will need to initialize weights to multiply with the inputs.
```{python}
g = torch.Generator().manual_seed(12343354351)
W = torch.randn((27,1), generator = g)
W.shape
```

Multiple inputs by weights.
If we have a weight matrix of dimensions 27X1, and we have 9 training examples of dimension 9X27, we should get a single number for each training example as a result of the operation W.x

The operator `@` in pytorch denotes matrix multiplication.

```{python}
xenc @ W
```

However, in this particular example, we don't want one output, we want 27 outputs, where each output can eventually be used to predict the probability of the next character. There are 27 possible next characters. Therefore, we need a W of dimension 27X27

```{python}

W = torch.randn((27,27), generator = g)
W.shape
```

The above can be thought of as a layer of 27 neurons.
Now multiply x and W to get 27 outputs for each training example

```{python}
xenc @ W, (xenc@W).shape
```

Each row of the matrix multiplication indicates the output of each of the 27 neurons for the training example on that row. For example, the 15th number in the 3rd row indicates the output of the 15th neuron for the 3rd input.

Now these output numbers can be thought of as log of counts. therefore, to get the counts, we will take an exponential of these numbers.

The exponential turns all numbers into a positive number. We can now normalize all 27 outputs for each example to get probabilities.
The transformation above is also called `softmax` transformation.
Softmax is kind of a normalization function that converts numbers into probabilities.

The code below represents one forward pass of the neural network.
```{python}
logits = (xenc @ W) # log-counts 
counts = logits.exp()
probs = counts / (counts.sum(dim = 1, keepdim=True))
probs, probs[0].sum()
```

Each column of each row of the tensor probs now represents the probability of the next character for the example represented by that row.

For example, row 0 of probs represents the probability of each of the 27 characters following the character "."

```{python}
probs[0]
```

Now we will tune the weights W so that the probabilities coming out is good, i.e., the loss function is low.

# Test the loop for gradient descent using a few bigram examples

```{python}

# initialize all the inputs and parameters
g = torch.Generator().manual_seed(12343354351)
xenc = F.one_hot(xs, num_classes = 27).float()
W = torch.randn(size = (27,27), requires_grad=True, generator = g) # If I do not have generator in the same cell, then at each run, the generator is not re-initialized. For reproducible results, it's important to have the generator object in the same cell so that it is re-initialized at each run of the cell.

for _ in range(50):
    # do a forward pass and calculate the loss
    logits = xenc @ W
    counts = logits.exp()
    probs = counts / counts.sum(dim = 1, keepdim=True)

    # Calculate the neg log likelihodd for the known next character in the training data
    # For first example
    nll = []
    for ix, example in enumerate(xs):
        #print(itos[example.item()], itos[ys[ix].item()], sep = "")
        #print(ys[ix])
        prob_true_char = probs[ix, ys[ix]]
        log_prob = prob_true_char.log()
        nll.append(-log_prob) # Do not use .item() here because pytorch needs to be able to track operations so that it can calculate gradients in the backward pass and it can only do that for tensors. If we use .item() to extract just the data, then pytorch cannot calculcate the backard pass any longer on the extracted data.
        #print(nll)


    loss = sum(nll)/len(nll)
    print(loss.item())

    # do a backward pass and calculate the gradient of the loss w.r.t. each parameter
    # make sure that all parameter gradients are zero before doing the backward pass
    W.grad = None
    loss.backward()
    W.grad


    # Update the parameters and repeat
    W.data += -1*W.grad # Do not update the entire W directly, instead update the data in as W.data. We will get an error if we update W directly because we will be modifying the whole tensor object in place and pytorch does not allow that.

```

```{python}
probs[8]
```



Make a more efficient loop 

# Initialize for all examples

```{python}
# initialize all the inputs and parameters
xs, ys = [], []

for word in words:
    chrs = "."+word+"."
    for ch1, ch2 in zip(chrs, chrs[1:]):
        ix1 = stoi[ch1]
        ix2 = stoi[ch2]
        #print(ch1, ch2, sep = "")
        xs.append(ix1)
        ys.append(ix2)
# Convert xs and ys to pytorch tensors
xs = torch.tensor(xs)
ys = torch.tensor(ys)
num = xs.nelement()
print("number of examples = ", num)


g = torch.Generator().manual_seed(12343354351)
xenc = F.one_hot(xs, num_classes = 27).float()
W = torch.randn(size = (27,27), requires_grad=True, generator = g)

```


# Run gradient descent for all examples

```{python}

for _ in range(500):
    # do a forward pass and calculate the loss
    logits = xenc @ W
    counts = logits.exp()
    probs = counts / counts.sum(dim = 1, keepdim=True)

    # Calculate the neg log likelihodd for the known next character in the training data

    loss = -probs[torch.arange(num), ys].log().mean()
    print(loss.item())
    
    # do a backward pass and calculate the gradient of the loss w.r.t. each parameter
    # make sure that all parameter gradients are zero before doing the backward pass
    W.grad = None
    loss.backward()


    # Update the parameters and repeat
    W.data += -50*W.grad 
```

What is the lower limit that loss should go down to?
The number we got from the count-based bigram model was around 2.27. Therefore, we expec tthe loss to go atleast to that number. We can see that the loss for this network stabilizes around 2.19

# Make predictions using the neural net

```{python}
gen = torch.Generator().manual_seed(347291378491)

for _ in range(20):
    char_list= ["."]
    ind = stoi["."]
    while True:
        p = probs[ind]
        # If all characters were equally likely
        #p = torch.ones(27)/27.0
        ind = torch.multinomial(input = p, num_samples = 1, replacement = True, generator=gen).item()
        #print(ind)
        #print(sample_ind)
        sample_char = itos[ind]
        char_list.append(sample_char)
        if ind == stoi["."]:
            break
    print(''.join(char_list))
```