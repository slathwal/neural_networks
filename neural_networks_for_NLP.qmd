---
title: "Learning to build NLP models using deep-learning"
author: "Shefali Lathwal"
date: 2025-06-07
date-modified: last-modified
format: html
echo: true
toc: true
jupyter: cs224n
---

Following along lectures from CS224n and Andrei Karpathy's Neural Networks: zero-to-hero series.

- Start with a character level language model

- Given a few characters, predict the next character in the sequence.

- We will start with character-level models and eventually go to word-level models.

- Start with bigram models that just predict a character given a single character
    - We should pay attention to characters that start and end words as well.

- Also think about torch summing operations

- Also think about sampling from multinomial distributions in torch

- Think about broadcasting rules/broadcasting semantics in torch
    - Look at tutorials for broadcasting and practice it

Step 1: Train a bigram character level model based on frequencies of character occurences in a training dataset.

Step 2: Train a neural network to predict the next character based on a training dataset.

# Get some data

Get some Indian names from a [website](https://www.acko.com/health-insurance/s/pregnancy/baby-names/modern-indian-baby-names-for-boys-and-girls-with-meanings/) and save them in a file called `names.txt`

```{python}
import re

with open("names.txt") as file:
    text = file.read().splitlines()
file.close()
# Remove empty strings from the list
text = [x for x in text if x]
# Remove the characters after '-'' as they are not part of the name and make all characters lowercase, and remove leading and training whitespaces
words = [word.split("-")[0].lower().strip() for word in text]
words, len(words), min(len(w) for w in words), max(len(w) for w in words)
```

# Get a list of character bigrams from the word list
```{python}
bigrams={}
for word in words:
    chs = '.'+word+'.'
    for ch1, ch2 in zip(chs, chs[1:]):
        key = ch1+ch2
        #print(key)
        bigrams[key] = bigrams.get(key,0)+1
        #print(bigrams)
        #print(ch1, ch2, sep = "")
#bigrams
```

```{python}
sorted(bigrams.items(), key = lambda kv: kv[1], reverse = True)[:10]
```

# Save the bigrams into a tensor so that we can later use pytorch
Since tensor has no labels, we will need a mapping from indices to characters and vice-versa

```{python}
all_chars = list(set(char for word in words for char in word))
# ANother way to do the above
all_chars = list(set("".join(words)))
#print(all_chars)
len(all_chars)
# Character x is missing and so is the end character "."
all_chars.append("x")
all_chars = sorted(all_chars)
all_chars, len(all_chars)

# stoi = {}
stoi = {s:i for i,s in enumerate(all_chars)}
stoi['.'] = 26
itos = {i:s for s,i in stoi.items()}
stoi, itos
```

Now, initialize an empty tensor to save all the bigram counts

```{python}
import torch

# starting with a baseline of one to avoid zero counts.
N = torch.ones((27,27), dtype = torch.int32)
N.shape

for word in words:
    chs = '.'+word+'.'
    for ch1, ch2 in zip(chs, chs[1:]):
        ix1, ix2 = stoi[ch1], stoi[ch2]
        N[ix1, ix2] += 1

#N
```


# Visualize the matrix
```{python}
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize = (10, 10))
im = ax.imshow(N)
ax.set_xticks(range(len(all_chars)+1), labels = all_chars+["."])
ax.set_yticks(range(len(all_chars)+1), labels = all_chars+["."])

# Loop over data dimensions and create text annotations
for i in range(27):
    for j in range(27):
        text = ax.text(j, i, N[i, j].item(), ha = "center", va = "top", color = "white")
        text = ax.text(j, i, itos[i]+itos[j], ha = "center", va = "bottom", color = "white")
fig.tight_layout()
```

# Calculate probability of a character following abother character

```{python}
character = 'a'
idx1 = stoi['a']
counts = N[idx1].float()
sum_counts = sum(counts)
#p = N[idx1]/sum(N[idx1])
counts, sum_counts
p = counts/sum_counts
p
```

# Calculate the tensor of probabilities
We want to divide the original counts with sums of each row. We have to follow broadcasting semantics of tensors and have to understand what operations are happening in the background.
```{python}
Np = N.float()
# The dimension along which sum is computed is reduced to size 1 in output tensor
# I want to sum across all rows. Therefore, I want sum along all rows in a single column
rowsum_tensor = Np.sum(dim = 1, keepdim=True)
rowsum_tensor.shape, rowsum_tensor
# One row has a count of zero. Therefore, we will add a count of 1 to any value that has a value of zero, to avoid a final division by zero
Np = (Np/rowsum_tensor)

fig, ax = plt.subplots(figsize = (10, 10))
im = ax.imshow(N)
ax.set_xticks(range(len(all_chars)+1), labels = all_chars+["."])
ax.set_yticks(range(len(all_chars)+1), labels = all_chars+["."])

# Loop over data dimensions and create text annotations
for i in range(27):
    for j in range(27):
        text = ax.text(j, i, round(Np[i, j].item(),2), ha = "center", va = "top", color = "white")
        text = ax.text(j, i, itos[i]+itos[j], ha = "center", va = "bottom", color = "white")
fig.tight_layout()
```

# Make predictions based on calculated probabilities
We will make predictions by sampling from the calculated probabilities using `torch.multinomial`
```{python}
#TO DO
```