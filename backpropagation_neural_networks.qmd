---
title: "Neural networks and back-propagation"
date: 2025-05-27
date-modified: last-modified
format: html
toc: true
echo: true
jupyter: cs224n
author: Shefali Lathwal
---

Following the ideas taken from a [lecture by Andrej Karpathy](https://www.youtube.com/watch?v=i94OvYb6noo) from the course Deep Learning for Computer Vision at Stanford, and Anrej Karpathy's course on [Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ) on Youtube.

- It is important to understand backpropagation rules, because many algorithms require you to write your own gradient functions.

A good rule of thumb is to write the gradient function, and then check that it is correct using numerical gradients.

In the first video of the course, Karpathy talks about micrograd, an engine he built from scratch that takes single scalar inputs and produce outputs using a neural network.

One of Karpathy's emphasis is the following by Richard Feynman:

::: {.callout-tip}
What I cannot create, I do not understand.

Know how to solve every problem that has been solved.
:::

I have known the above intuitively, but seeing it spelled out is extremely useful and lays out clearly why understanding the theory behind things has always been important to me. The piece that I missed in recent years is the ability to create things yourself. Learning and learning enough to create yourself are two very different things and the latter is where learning truly happens, that gives mastery over a subject, and that gives joy.

In this notebook, I am creating the code from Karpathy's first lecture in Zero to Hero series on my own, using classes to build and train a simple neuron and then a multi-layer percepteron. I will also use pytorch to compare my implementation with the pytorch implementation.

::: {.callout-note}
Important things to remember:
- Do not overwrite the gradients of an object. Instead add to the existing gradients so that we can handle the case of two same objects in an operation.
- Because we are adding the gradients instead of overwriting them, remember to zero the gradients before each back propagation step.
:::

# Define a class that keeps track of data, gradients and operations

```{python}
import numpy as np
class Value():
    "A class to store single values and their gradients that can be used to build neural networks"

    # We need to track which elements the current object depends on
    def __init__(self, data, _children = (), _op = "", label =''):
        self.data = data
        self._prev = set(_children)
        self._op = _op
        self.label = label
        self.grad = 0.0 # default gradient is zero, i.e., node has no effect on output
        self._backward = lambda: None # by default it will be an empty function
        
    def __repr__(self):
        "Add a string representation of the class"
        return f'Value(data: {self.data}, operator: {self._op})'

    def __add__(self, other):
        out = Value(self.data + other.data, _children = (self, other), _op = "+")
        
        def calculate_back_gradient():
            # If we call the grads outside this function, gradients will be zero, because by default out.grad is zero
            self.grad += 1.00*out.grad
            other.grad += 1.00*out.grad
        
        out._backward = calculate_back_gradient # function that propagates the gradient
        return out

    def __mul__(self, other):
        out = Value(self.data * other.data, _children = (self, other), _op = "*")
        def calculate_back_gradient():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = calculate_back_gradient
        return out

    def tanh(self):
        out = Value(np.tanh(self.data), _children = (self,), _op = "tanh") # In the video Karpathy used exponentials, and I am using the function directly from numpy. 
        def calculate_back_gradient():
            self.grad += (1 - out.data**2)* out.grad
        out._backward = calculate_back_gradient
        return out

    def backward(self):
        topo = []
        visited = set()
        def build_topo(node):
            if node not in visited:
                visited.add(node)
                for child in node._prev:
                    build_topo(child)
                topo.append(node)
        build_topo(self)

        self.grad = 1.0
        for node in reversed(topo):
            node._backward()

```

```{python}
import numpy as np
import math
import matplotlib.pyplot as plt
x = np.arange(-10, 11, 0.2) 
y = np.tanh(x) # math.tanh function only takes a single number and not an array.
plt.plot(x, y)
plt.grid()
```

Notes:
- `__repr__` is a function in a class that provides a nice string representation of the objects created from that class.
- The arguments in init and the names of variables and data structures used to store those arguments can be different. For example, in the class `Value`, we use an argument called `_children`, which is a tuple, and save a variable called `_prev` as a `set(_children)`
- We give simple text names to arguments that we provide while creating an object or calling a method on an object from a class definition. However, if there are arguments that are created during function calls or other internal operations using objects of a class, we tend to prefix those names with an underscore. For example, see `_op`, `_children`. 
- The above goes for names of methods in a class as well. If we want users to call a method explicitly, we give it a simple text name. However, if there are internal methods or methods that are called by other functions, we tend to prefix names with an underscore.

```{python}
a = Value(2.0, label = "a")
b = Value(3.0, label = "b")
c = a*b; c.label = 'c'
d = Value(-1.0, label = 'c')
e = c + d; e.label = "e"

f = c+c; f.label = "f"
f._prev

g = a + b + c; g.label="g"
g._prev
type(id(c))
f.grad
```

# Visualize computational graphs using graphviz

```{python}
from graphviz import Digraph
def trace(root):
    nodes, edges = set(), set()
    def build(v):
        #print("\ncalling build for:", v)
        if v not in nodes:
            #print("add node for:", v)
            nodes.add(v)
            #print("children:", v._prev)
            for child in v._prev:
                #print("add child:",child)
                edges.add((child, v)) # Add a tuple going from child to parent
                build(child)

    build(root)
    return nodes, edges
#trace(f)
def draw_dot(root):
    graph = Digraph(graph_attr={'rankdir': 'LR'})
    nodes, edges = trace(root)

    for n in nodes:
        uid = str(id(n))
        graph.node(name = uid, label = '{%s | {data: %.4f | grad:%0.4f}} ' %(n.label, n.data, n.grad), shape = "record")
        # If n has an operator:
        if n._op:
            graph.node(name = uid+n._op, label = n._op)
            # ccreate an edge from operator node to this node's data
            graph.edge(uid+n._op, uid)

    for n1, n2 in edges:
        "Connect n1 to op node of n2"
        graph.edge(str(id(n1)), str(id(n2))+n2._op)

    return graph

```

Note: If by the `rankdir` is `LR` for a node of type `record`, then the label placement by default is horizontal if labels are separated by a vertical bar. However, if the label string is enclosed in curly brackets, the placement is vertical when labels are separated by a vertical bar.

```{python}
trace(f)
display(draw_dot(f))

display(draw_dot(e))
display(draw_dot(g))
```

- We can visualize computational graphs built using the objects from class `Value`. Now it's time to create local gradients and figure out how to do back_propagation.

# Calculate gradients and perform back-propagation for a simple computation
```{python}
a = Value(2.0, label = "a")
b = Value(3.0, label = "b")
c = a*b; c.label = "c"
d = Value(-1.0, label = "d")
e = c + d; e.label = "e"
f = Value(-2.0, label = "f")
L = e*f; L.label = "L"
#display(draw_dot(L)) # visualize the forward pass of the computation
L
```

```{python}
display(draw_dot(L))
```

If you keep running the backward function again without zeroing the grads, the gradients will just keep adding

```{python}
L.grad = 1

L._backward()
e._backward()
c._backward()
d._backward()
print(a.grad, b.grad, c.grad, d.grad, e.grad, f.grad)
```

# Define one neuron

```{python}
# inputs
x1 = Value(2.0, label = 'x1')
x2 = Value(0.0, label = "x2")

# weights
w1 = Value(-3.0, label = 'w1')
w2 = Value(1.0, label = 'w2')

# bias term
b = Value(6.88137, label = 'b')

#x1w1 + x2w2 +b
x1w1 = x1*w1; x1w1.label = 'x1w1'
x2w2 = x2*w2; x2w2.label = 'x2w2'
x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1x2w2'
n = x1w1x2w2 + b; n.label = 'n'
o = n.tanh(); o.label = 'o'

```

```{python}
draw_dot(o)
```

# Peform backpropagation on the neuron
In a neural netowrk, we are interested in gradients with respect to weights and bias so that we can backpropagate

We never want to call `._backward` function on any note before we have done everythong after it. So we need to order the nodes to know which ones are before and which ones are after. This ordering can be done using topological sort, which is a laying out of nodes so that all edges go in one direction, e.g. from left to right. We have added this function to the Value class

```{python}
o.backward()
```

```{python}
draw_dot(o)
```
