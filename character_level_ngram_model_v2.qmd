---
title: Developing a character-prediction n-gram model using MLP - version 2
author: Shefali Lathwal
date: 2025-06-12
date-modified: last-modified
format: html
jupyter: cs224n
echo: true
toc: true
---

In this notebook, we are going to improve on the character-level n-gram model that we built in version 1. In particular we are going to do the following:
- Implement gradient descent on a mini-batch for the data instead of the whole dataset at once.
- Split the data into training, validation and test sets
- Visualize the final embedding vectors predicted by the model
- Compare performance of the model from v1

# Build the training dataset
Just like the Bengio et al. paper, let's build a training dataset with three characters in context.

- Remember that we have "." on our training data itself. Therefore we are using a * to denote the beginning and end of words.

- Our context is not a single character but a sequence of three characters. So each row in our xs training set would ba a list of three indices, and ys would be a single index
```{python}
with open("data/names.txt") as file:
    text = file.read().splitlines()
file.close()
words = text
len(words)

# Create the lookup for character indices including a start and end of line character to the all_chars list

all_chars = sorted(list(set("".join(words)))+["*"])

stoi = {s:i for i, s in enumerate(all_chars)}
itos  = {i:s for i, s in stoi.items()}
stoi
```

## Set some parameters

```{python}
vocab_size = len(all_chars) # vocabulary size for characters
context_length = 3 # no of characters in context for prediction
# embedding vector length for each character
embed_size = 2
n_hidden = 100 # number of neurons in the hidden layer

# min_batch_size = 32 # currently being set later in the notebook, move here.
```


```{python}
import torch
import torch.nn.functional as F
xs, ys = [], []

for word in words:
    chs = "*"*context_length+word+"*"
    #print(word)
    for ind in range(len(chs) - context_length):
        ch1 = chs[ind:ind+context_length]
        ch2 = chs[ind+context_length]
        #print(f"{ch1} ----> {ch2}")
        xs.append([stoi[ch] for ch in ch1])
        ys.append(stoi[ch2])


xs = torch.tensor(xs) # 10X3
ys = torch.tensor(ys) # 10

num = len(ys)
print("number of examples:", num)
```


# Train in minibatches instead of the whole dataset
We will sample a minibatch from the data instead of taking the whole data at every iteration of the gradient descent algorithm.

Initialize
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True
```

Train
```{python}
n_iters = 1000
min_batch_size = 32
for _ in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, num, (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xs[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ys[ix])

    #print(f"{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()

    # Update parameters
    for p in parameters:
        p.data += -0.1*p.grad

print(loss.item())
```

The above loss is just for the minibatch
Calculating loss for the entire data
```{python}
xemb = C[xs].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ys)
print(loss.item())
```

- Training loss after 1000 iterations with a learning rate of 0.1 for the entire dataset, where gradient descent is run on the entire dataset = 2.3811113834381104

- Training loss after 1000 iterations with a learning rate of 0.1 for the entire dataset, where the gradient descent is run with mini-batch size of 32 = 2.5501720905303955, but the training is really fast, so we can go for much longer iterations.

# Tuning the learning rate

- Find a range empirically that works well. For example from 0.001 to 1. Find this range by just manually setting the rate for a few iterations and finding what the upper and lower limits are likely to be.
- Run iterations slowly increasing the learning rate with each iteration over the range
- Look at where the loss starts to increase
- It will be a reasonable estimation of the learning rate.

Initialize
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True
```

Define a range to
```{python}
lre = torch.linspace(-3, 0, 1000)
lri = 10**lre
lri[:10]
```

Train
```{python}
n_iters = 1000
lossi = []
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, num, (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xs[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ys[ix])

    #print(f"{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    lossi.append(loss.item())
    # Update parameters
    lr = lri[i]
    for p in parameters:
        p.data += -lr*p.grad

#print(loss.item())
```

Plot the loss with the learning rate
```{python}
import matplotlib.pyplot as plt
import numpy as np
plt.plot(lre, lossi)
plt.ylabel("loss")
plt.xlabel("learning rate exponent")

# index for min loss
ind_min_loss = np.argmin(lossi)
ind_min_loss
learning_rate = lri[ind_min_loss]
learning_rate
```

The learning rate has come out to be 0.18


Initialize
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True
```

Train
```{python}
n_iters = 100000
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, num, (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xs[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ys[ix])

    if i%10000 == 0:
        print(f"Loss at iteration {i}:{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad
print(f"{loss.item()}")

#The above loss is just for the minibatch
#Calculating loss for the entire data
xemb = C[xs].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ys)
print("Loss for the whole dataset:", loss.item())
```

10000 iterations is not enough to completely train the model. It needs more iterations. Also, I have to reduce the learning rate after 10,000 iterations fo rthe model to continue converging smoothly.

The above loss is on the entire training dataset and we may be overfitting the training data. Therefore, to get a reasonable assessment of the loss of the model on unseen data, we should split our data into training, dev and test sets.

# Split the data into training, validation and test sets

- Define a function to build the X and Y tensors from the words list.

- Define two integers, n1 and n2 at 80% of the word list and 90% of the word list. 

- Shuffle the words and extract data upto n1, from n1 to n2, and from n2 to end. These will become the training, dev and test datasets.

```{python}
#TODO
```

# Visualize the embedding vectors
```{python}
#TODO
```


# Sample from the model

```{python}
start_char = "*"
g = torch.Generator().manual_seed(123434)

for _ in range(20):
    chs = [ch for ch in start_char*context_length]
    #print(chs)
    all_chars = chs
    nump = 1
    while True:  
        #print(chs)
        xp = []
        xp.append([stoi[ch] for ch in chs])
        #print(xs)
        xembp = C[xp] # 1X3X2
        xembp = xembp.view(nump, -1) #1X6
        xlp = xembp @ W1 + b1 
        xhp = xlp.tanh() # 1X100
        logitsp = xhp @ W2 + b2 #1X30
        countsp = logitsp.exp()
        probsp = countsp / countsp.sum(dim = 1, keepdim = True) #1X30
        ind = torch.multinomial(probsp[0], num_samples = 1, replacement = True, generator = g)
        #print(ind)
        ch = itos[ind.item()]
        #print(ch)
        if (ch == "*"):
            all_chars.append(ch)
            break
        else:   
            all_chars.append(ch)
            chs = all_chars[-context_length:]
            #print(chs)
    print("".join(all_chars))
```
