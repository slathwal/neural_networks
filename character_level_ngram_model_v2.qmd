---
title: Developing a character-prediction n-gram model using MLP - version 2
author: Shefali Lathwal
date: 2025-06-12
date-modified: last-modified
format: html
jupyter: cs224n
echo: true
toc: true
---

In this notebook, we are going to improve on the character-level n-gram model that we built in version 1. In particular we are going to do the following:
- Implement gradient descent on a mini-batch for the data instead of the whole dataset at once.
- Split the data into training, validation and test sets
- Visualize the final embedding vectors predicted by the model
- Compare performance of the model from v1

# Build the training dataset
Just like the Bengio et al. paper, let's build a training dataset with three characters in context.

- Remember that we have "." on our training data itself. Therefore we are using a * to denote the beginning and end of words.

- Our context is not a single character but a sequence of three characters. So each row in our xs training set would ba a list of three indices, and ys would be a single index
```{python}
with open("data/names.txt") as file:
    text = file.read().splitlines()
file.close()
words = text
len(words)

# Create the lookup for character indices including a start and end of line character to the all_chars list

all_chars = sorted(list(set("".join(words)))+["*"])

stoi = {s:i for i, s in enumerate(all_chars)}
itos  = {i:s for s, i in stoi.items()}
stoi, itos
```

## Set some parameters

```{python}
vocab_size = len(all_chars) # vocabulary size for characters
context_length = 3 # no of characters in context for prediction
# embedding vector length for each character
embed_size = 2
n_hidden = 100 # number of neurons in the hidden layer

# min_batch_size = 32 # currently being set later in the notebook, move here.
```


```{python}
import torch
import torch.nn.functional as F
xs, ys = [], []

for word in words:
    chs = "*"*context_length+word+"*"
    #print(word)
    for ind in range(len(chs) - context_length):
        ch1 = chs[ind:ind+context_length]
        ch2 = chs[ind+context_length]
        #print(f"{ch1} ----> {ch2}")
        xs.append([stoi[ch] for ch in ch1])
        ys.append(stoi[ch2])


xs = torch.tensor(xs) # 10X3
ys = torch.tensor(ys) # 10

num = len(ys)
print("number of examples:", num)
```


# Train in minibatches instead of the whole dataset
We will sample a minibatch from the data instead of taking the whole data at every iteration of the gradient descent algorithm.

Initialize
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True
```

Train
```{python}
n_iters = 1000
min_batch_size = 32
for _ in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, num, (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xs[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ys[ix])

    #print(f"{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()

    # Update parameters
    for p in parameters:
        p.data += -0.1*p.grad

print(loss.item())
```

The above loss is just for the minibatch
Calculating loss for the entire data
```{python}
xemb = C[xs].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ys)
print(loss.item())
```

- Training loss after 1000 iterations with a learning rate of 0.1 for the entire dataset, where gradient descent is run on the entire dataset = 2.3811113834381104

- Training loss after 1000 iterations with a learning rate of 0.1 for the entire dataset, where the gradient descent is run with mini-batch size of 32 = 2.5501720905303955, but the training is really fast, so we can go for much longer iterations.

# Tuning the learning rate

- Find a range empirically that works well. For example from 0.001 to 1. Find this range by just manually setting the rate for a few iterations and finding what the upper and lower limits are likely to be.
- Run iterations slowly increasing the learning rate with each iteration over the range
- Look at where the loss starts to increase
- It will be a reasonable estimation of the learning rate.

Initialize
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True
```

```{python}
lre = torch.linspace(-3, 0, 1000)
lri = 10**lre
lri[:10]
```

Train
```{python}
n_iters = 1000
lossi = []
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, num, (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xs[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ys[ix])

    #print(f"{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    lossi.append(loss.item())
    # Update parameters
    lr = lri[i]
    for p in parameters:
        p.data += -lr*p.grad

#print(loss.item())
```

Plot the loss with the learning rate
```{python}
import matplotlib.pyplot as plt
import numpy as np
plt.plot(lre, lossi)
plt.ylabel("loss")
plt.xlabel("learning rate exponent")

# index for min loss
ind_min_loss = np.argmin(lossi)
ind_min_loss
learning_rate = lri[ind_min_loss]
learning_rate
```

The learning rate has come out to be 0.18


Initialize
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True
```

Train
```{python}
n_iters = 100000
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, num, (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xs[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ys[ix])

    if i%10000 == 0:
        print(f"Loss at iteration {i}:{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad
print(f"{loss.item()}")

#The above loss is just for the minibatch
#Calculating loss for the entire data
xemb = C[xs].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ys)
print("Loss for the whole dataset:", loss.item())
```

10000 iterations is not enough to completely train the model. It needs more iterations. Also, I have to reduce the learning rate after 10,000 iterations fo rthe model to continue converging smoothly.

The above loss is on the entire training dataset and we may be overfitting the training data. Therefore, to get a reasonable assessment of the loss of the model on unseen data, we should split our data into training, dev and test sets.

# Split the data into training, validation and test sets

- Define a function to build the X and Y tensors from the words list.

- Define two integers, n1 and n2 at 80% of the word list and 90% of the word list. 

- Shuffle the words and extract data upto n1, from n1 to n2, and from n2 to end. These will become the training, dev and test datasets.

```{python}
import random
def build_dataset(words, context_length = context_length):
    xs, ys = [], []
    for word in words:
        chs = "*"*context_length+word+"*"
        #print(word)
        for ind in range(len(chs) - context_length):
            ch1 = chs[ind:ind+context_length]
            ch2 = chs[ind+context_length]
            #print(f"{ch1} ----> {ch2}")
            xs.append([stoi[ch] for ch in ch1])
            ys.append(stoi[ch2])

    xs = torch.tensor(xs) # 10X3
    ys = torch.tensor(ys) # 10
    print(xs.shape, ys.shape)
    return xs, ys

random.shuffle(words)
n1 = int(0.8*len(words))
n2 = int(0.9*len(words))
n1, n2

xstr, ystr = build_dataset(words[:n1])
xsdev, ysdev = build_dataset(words[n1:n2])
xstest, ystest = build_dataset(words[n2:])
```

Initialize
```{python}
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True

print("Number of parameters:", sum(p.nelement() for p in parameters))
```

Train
```{python}
n_iters = 100000
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, xstr.shape[0], (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xstr[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ystr[ix])

    if i%10000 == 0:
        print(f"Loss at iteration {i}:{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad
print(f"{loss.item()}")

#The above loss is just for the minibatch
#Calculating loss for the entire training data
xemb = C[xstr].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ystr)
print("Training loss:", loss.item())

# Calculate loss for the dev data
xemb = C[xsdev].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ysdev)
print("Validation loss:", loss.item())
```

The training and validation loss are quite close to each other, being 2.11 and 2.12, respectively after 100,000 iterations with the above learning rates.

When training loss and validation loss are very similar, it indicates that our neural network is not overfitting on the training data, and there should be scope for improving it. Some strategies that we can use:

1. Increase the number of parameters in the neural network by increasing the number of neurons in the hidden layer.
2. Increase the embed_size to capture more information for each character
3. We can also take more than three characters in context

# Experiment - increase the parameters in the hidden layer

Initialize
```{python}
n_hidden = 300
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True

print("Number of parameters:", sum(p.nelement() for p in parameters))
```

Train
```{python}
n_iters = 100000
lossi = []
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, xstr.shape[0], (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xstr[ix]].view(min_batch_size, context_length*embed_size) # n_examplesX6
    xh = (xemb @ W1 + b1).tanh() # n_examplesX100
    logits = xh @ W2 + b2 # n_examplesXvocab_size
    loss = F.cross_entropy(logits, ystr[ix])

    if i%10000 == 0:
        print(f"Loss at iteration {i}:{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad

    # track statistics
    lossi.append(loss.item())
print(f"{loss.item()}")

#The above loss is just for the minibatch
#Calculating loss for the entire training data
xemb = C[xstr].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ystr)
print("Training loss:", loss.item())

# Calculate loss for the dev data
xemb = C[xsdev].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ysdev)
print("Validation loss:", loss.item())
```
The training and dev losses are now 2.09 and 2.10 respectively after 100,000 steps.

```{python}
plt.plot(np.arange(n_iters), lossi)
plt.xlabel("iteration")
plt.ylabel("loss")
```

One of the reasons why the line is thick is because our batch size is quite small at 32. Therefore, there is a lot of jitter in the loss value.

# Visualize the 2-D embedding vectors
```{python}
embed_1 = C[:, 0].detach().numpy() # embedding in 1st dimension
embed_2 = C[:,1].detach().numpy() # embedding in 2nd dimension
# plt.plot(C[:,0], C[:,1]) # This line creates an error
plt.scatter(embed_1, embed_2, s = 200)

for i in range(C.shape[0]):
    #print(i)
    plt.text(embed_1[i], embed_2[i],s = itos[i], ha = "center", va = "center", color = "white")
plt.grid("minor")
```

We can see that the vowels cluster together on the center right.

- The start and end character clusters with the empty character.

# Experiment - Increase the embedding vector size to 10

Initialize
```{python}
n_hidden = 300
embed_size = 10
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True

print("Number of parameters:", sum(p.nelement() for p in parameters))
```

Train
```{python}
n_iters = 100000
lossi = []
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, xstr.shape[0], (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xstr[ix]].view(min_batch_size, context_length*embed_size)
    xh = (xemb @ W1 + b1).tanh() 
    logits = xh @ W2 + b2 
    loss = F.cross_entropy(logits, ystr[ix])

    if i%10000 == 0:
        print(f"Loss at iteration {i}:{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad

    # track statistics
    lossi.append(loss.item())
print(f"{loss.item()}")

#The above loss is just for the minibatch
#Calculating loss for the entire training data
xemb = C[xstr].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ystr)
print("Training loss:", loss.item())

# Calculate loss for the dev data
xemb = C[xsdev].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ysdev)
print("Validation loss:", loss.item())
```

The training and dev losses are now 2.044 and 2.069 respectively after 100,000 steps.

```{python}
plt.figure()
plt.plot(np.arange(n_iters), lossi)
plt.xlabel("iteration")
plt.ylabel("loss")

# It can also be useful to plot logloss instead of loss
plt.figure()
plt.plot(np.arange(n_iters), np.log10(lossi))
plt.xlabel("iteration")
plt.ylabel("Log loss")
```

# Sample from the model

```{python}
start_char = "*"
end_char = "*"
g = torch.Generator().manual_seed(123434)
start_ind = stoi[start_char]
for _ in range(20):
    context = [start_ind]*context_length
    all_chars = [start_char]*context_length
    while True:  
        # Now context contains the three character indices for our example
        xembp = C[torch.tensor(context)].view(1, -1) # Want to generate just one character at a time.
        xhp = (xembp @ W1 + b1).tanh() 
        logitsp = xhp @ W2 + b2 
        # Directly use the softmax function to calculate probabilites from logits
        probsp = F.softmax(logitsp, dim = 1)
        ind = torch.multinomial(probsp, num_samples = 1, replacement = True, generator = g)
        ch = itos[ind.item()]
        if (ch == end_char):
            all_chars.append(ch)
            break
        else:   
            all_chars.append(ch)
            context = context[1:]+[ind.item()]
    print("".join(all_chars))
```


#TODO - Think about how to visualize a n-dimensional embedding vector - refer to assignment 1 from CS224n

# Experiment = Increase the context length to 6

Rebuild the training, dev and test sets
```{python}
context_length = 6

random.shuffle(words)
n1 = int(0.8*len(words))
n2 = int(0.9*len(words))
n1, n2

xstr, ystr = build_dataset(words[:n1], context_length=context_length)
xsdev, ysdev = build_dataset(words[n1:n2], context_length=context_length)
xstest, ystest = build_dataset(words[n2:], context_length=context_length)
```

Initialize
```{python}

n_hidden = 300
embed_size = 10
g = torch.Generator().manual_seed(4524757136458)
C = torch.randn(size = (vocab_size, embed_size), generator=g) # 30 X 2

W1 = torch.randn(size = (context_length * embed_size,n_hidden), generator=g) # 6X100
b1 = torch.randn(size = (n_hidden,), generator=g) # 100

W2 = torch.randn(size = (n_hidden, vocab_size), generator=g) #100X30
b2 = torch.rand(size = (vocab_size,), generator=g) #30

parameters = [C, W1, b1, W2, b2]
# collect all the parameters
for p in parameters:
    p.requires_grad = True

print("Number of parameters:", sum(p.nelement() for p in parameters))
```

Train
```{python}
n_iters = 100000
lossi = []
for i in range(n_iters):
    
    # construct minibatch by sampling indices from training dataset
    ix = torch.randint(0, xstr.shape[0], (min_batch_size,), generator=g)
    
    # forward pass
    xemb = C[xstr[ix]].view(min_batch_size, context_length*embed_size)
    xh = (xemb @ W1 + b1).tanh() 
    logits = xh @ W2 + b2 
    loss = F.cross_entropy(logits, ystr[ix])

    if i%10000 == 0:
        print(f"Loss at iteration {i}:{loss.item()}")
    
    # Backward pass
    for p in parameters:
        p.grad = None
    loss.backward()
    # Update parameters
    if i < 10000:
        lr = learning_rate
    elif 10000 <= i < 50000:
        lr = learning_rate/10
    else:
        lr = learning_rate/100
    for p in parameters:
        p.data += -lr*p.grad

    # track statistics
    lossi.append(loss.item())
print(f"{loss.item()}")

#The above loss is just for the minibatch
#Calculating loss for the entire training data
xemb = C[xstr].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ystr)
print("Training loss:", loss.item())

# Calculate loss for the dev data
xemb = C[xsdev].view(-1, context_length*embed_size) # n_examplesX6
xh = (xemb @ W1 + b1).tanh() # n_examplesX100
logits = xh @ W2 + b2 # n_examplesXvocab_size
loss = F.cross_entropy(logits, ysdev)
print("Validation loss:", loss.item())
```

The training and dev losses are now 2.064 and 2.083 respectively after 100,000 steps.
The losses have actually increased by increasing the context length. Therefore, context length is not likely a bottleneck in our model.


```{python}
plt.figure()
plt.plot(np.arange(n_iters), lossi)
plt.xlabel("iteration")
plt.ylabel("loss")

# It can also be useful to plot logloss instead of loss
plt.figure()
plt.plot(np.arange(n_iters), np.log10(lossi))
plt.xlabel("iteration")
plt.ylabel("Log loss")
```


# Sample from the model

```{python}
start_char = "*"
end_char = "*"
g = torch.Generator().manual_seed(123434)
start_ind = stoi[start_char]
for _ in range(20):
    context = [start_ind]*context_length
    all_chars = [start_char]*context_length
    while True:  
        # Now context contains the three character indices for our example
        xembp = C[torch.tensor(context)].view(1, -1) # Want to generate just one character at a time.
        xhp = (xembp @ W1 + b1).tanh() 
        logitsp = xhp @ W2 + b2 
        # Directly use the softmax function to calculate probabilites from logits
        probsp = F.softmax(logitsp, dim = 1)
        ind = torch.multinomial(probsp, num_samples = 1, replacement = True, generator = g)
        ch = itos[ind.item()]
        if (ch == end_char):
            all_chars.append(ch)
            break
        else:   
            all_chars.append(ch)
            context = context[1:]+[ind.item()]
    print("".join(all_chars))
```
