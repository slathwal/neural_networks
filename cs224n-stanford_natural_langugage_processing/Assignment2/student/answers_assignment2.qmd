---
title: "CS224n assignment2: neural networks, backpropagation, pytorch, dependency parsing"
author: Shefali Lathwal
date: 2025-05-28
toc: true
format: html
jupyter: cs224n
echo: true
---

# 1. Understanding `word2vec`

Q1. Prove that the naive-softmax loss $J_{naive-softmax}(vc, o, U) = âˆ’ log P(O = o|C = c)$ is the same as the cross-entropy loss between $y$ and $\hat{y}$, i.e. (note that $y$ (true distribution), $\hat{y}$ (predicted distribution) are vectors and $\hat{y}_{o}$ is a scalar):



Ans: The cross entropy loss between $y$ and $\hat{y}$ is given as follows: $$- \sum_{w \in Vocab}{y_w log(\hat{y}_w)} = -log(\hat{y}_o) $$

For $w \ne o$, $y_w = 0$, therefore, all the terms inside the summation become 0.

For $w = o$, $y_w = y_o = 1$,which is the only remaining term inside the summation.
Since $P(O = o|C = c)$ is the conditional probability of the outide word being $o$, given a center word $c$, it is equivalent to $\hat{y}_o$. Therefore, the naive softmax loss is equal to the cross-entropy loss between $y$ and $\hat{y}_o$ 

