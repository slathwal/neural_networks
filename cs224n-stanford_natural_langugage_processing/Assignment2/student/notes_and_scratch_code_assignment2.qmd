---
title: "Scratch notebook - CS224n assignment2: neural networks, backpropagation, pytorch, dependency parsing"
author: Shefali Lathwal
date: 2025-05-28
toc: true
format: html
jupyter: cs224n
echo: true
---

# Notes
One of the interesting errors I ran into was that my model was breaking while calculating dependencies. The error was happening because two sentences finished parsing at the same time. However, because I was modifying the list unfinished_parses in-place in the file `parser_transitions.py` in place, when two lists finished at the same time, one of the parses failed to be removed as the finished parses were at positions 0 and 1 of the unfinished_parses list. 

I was able to resolve the bug by using list comprehension to include the unfinished parses rather than removing the parses in-place from the list.

# 1. Understanding `word2vec`

Q1. Prove that the naive-softmax loss $J_{naive-softmax}(vc, o, U) = âˆ’ log P(O = o|C = c)$ is the same as the cross-entropy loss between $y$ and $\hat{y}$, i.e. (note that $y$ (true distribution), $\hat{y}$ (predicted distribution) are vectors and $\hat{y}_{o}$ is a scalar):



Ans: The cross entropy loss between $y$ and $\hat{y}$ is given as follows: $$- \sum_{w \in Vocab}{y_w log(\hat{y}_w)} = -log(\hat{y}_o) $$

For $w \ne o$, $y_w = 0$, therefore, all the terms inside the summation become 0.

For $w = o$, $y_w = y_o = 1$,which is the only remaining term inside the summation.
Since $P(O = o|C = c)$ is the conditional probability of the outide word being $o$, given a center word $c$, it is equivalent to $\hat{y}_o$. Therefore, the naive softmax loss is equal to the cross-entropy loss between $y$ and $\hat{y}_o$ 


# Test code
```{python}
import re
sentence = "I am a big person!!!"

sentence_list = [re.sub(r'[\W]','',w) for w in sentence.split(" ")]
#sentence_list.reverse()
sentence_list

stack = ["ROOT"]
```

Shift Transition
```{python}
stack.append(sentence_list.pop(0)), sentence_list, stack
```

Left Arc Transition

```{python}
second_word = stack.pop(-2)
first_word = stack[-1]
transition = (first_word, second_word)
print(stack, transition, sentence_list)
```

# Check if a list is empty
```{python}
test_list = [1,2,3]
while test_list:
    print(test_list.pop(-1))
```

